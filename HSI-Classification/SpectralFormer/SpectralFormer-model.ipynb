{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YpV8t0NcPTcDxjv78V2x8WqsNDBGkyTC","authorship_tag":"ABX9TyOabdnKN9/aiiLb1IXWpVie"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcYITLJQndJf","executionInfo":{"status":"ok","timestamp":1680939547312,"user_tz":-480,"elapsed":5346,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"c3430489-9892-4792-aba5-4a7df0fef100"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.0\n"]}]},{"cell_type":"code","execution_count":29,"metadata":{"id":"cIp2OtQhjWxK","executionInfo":{"status":"ok","timestamp":1680940853288,"user_tz":-480,"elapsed":1020,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from einops import rearrange, repeat\n","\n","class Residual(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(x, **kwargs) + x\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads, dim_head, dropout):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x, mask = None):\n","        # x:[b,n,dim]\n","        b, n, _, h = *x.shape, self.heads\n","\n","        # get qkv tuple:([b,n,head_num*head_dim],[...],[...])\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        # split q,k,v from [b,n,head_num*head_dim] -> [b,head_num,n,head_dim]\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n","\n","        # transpose(k) * q / sqrt(head_dim) -> [b,head_num,n,n]\n","        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n","        mask_value = -torch.finfo(dots.dtype).max\n","\n","        # mask value: -inf\n","        if mask is not None:\n","            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n","            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n","            mask = mask[:, None, :] * mask[:, :, None]\n","            dots.masked_fill_(~mask, mask_value)\n","            del mask\n","\n","        # softmax normalization -> attention matrix\n","        attn = dots.softmax(dim=-1)\n","        # value * attention matrix -> output\n","        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n","        # cat all output -> [b, n, head_num*head_dim]\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        out = self.to_out(out)\n","        return out\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_head, dropout, num_channel, mode):\n","        super().__init__()\n","        \n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n","                Residual(PreNorm(dim, FeedForward(dim, mlp_head, dropout = dropout)))\n","            ]))\n","\n","        self.mode = mode\n","        self.skipcat = nn.ModuleList([])\n","        for _ in range(depth-2):\n","            self.skipcat.append(nn.Conv2d(num_channel+1, num_channel+1, [1, 2], 1, 0))\n","\n","    def forward(self, x, mask = None):\n","        #print(\"***********************************************\")\n","        #print(\"transformer model has action\")\n","        #print(self.layers)\n","        if self.mode == 'ViT':\n","            for attn, ff in self.layers:\n","                x = attn(x, mask = mask)\n","                x = ff(x)\n","        elif self.mode == 'CAF':\n","            last_output = []\n","            nl = 0\n","            for attn, ff in self.layers:           \n","                last_output.append(x)\n","                if nl > 1:\n","                    #print(self.skipcat[nl-2])             \n","                    x = self.skipcat[nl-2](torch.cat([x.unsqueeze(3), last_output[nl-2].unsqueeze(3)], dim=3)).squeeze(3)\n","                    #这里便是论文中所说的CAF模块，他是先将两个token融合后在进行了一次卷积！！！！！！！！！！！\n","                    #这里便是论文中所说的CAF模块，他是先将两个token融合后在进行了一次卷积！！！！！！！！！！！\n","                    #这里便是论文中所说的CAF模块，他是先将两个token融合后在进行了一次卷积！！！！！！！！！！！\n","                    #这里便是论文中所说的CAF模块，他是先将两个token融合后在进行了一次卷积！！！！！！！！！！！\n","                    #这里便是论文中所说的CAF模块，他是先将两个token融合后在进行了一次卷积！！！！！！！！！！！\n","                    #这里便是论文中所说的CAF模块，他是先将两个token融合后在进行了一次卷积！！！！！！！！！！！\n","                x = attn(x, mask = mask)\n","                x = ff(x)\n","                nl += 1\n","\n","        return x\n","\n","class ViT(nn.Module):\n","    def __init__(self, image_size, near_band, num_patches, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=1, dim_head = 16, dropout=0., emb_dropout=0., mode='ViT'):\n","        super().__init__()\n","\n","        patch_dim = image_size ** 2 * near_band\n","        \n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","\n","        self.dropout = nn.Dropout(emb_dropout)\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout, num_patches, mode)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","    def forward(self, x, mask = None):\n","       \n","        # patchs[batch, patch_num, patch_size*patch_size*c]  [batch,200,145*145]\n","        #print(\"输入的初始数据下x.shape:\",x.shape)\n","        x = torch.squeeze(x, dim=1)\n","        x = rearrange(x, 'b c h w -> b c (h w)')\n","\n","        ## embedding every patch vector to embedding size: [batch, patch_num, embedding_size]\n","        #print(\"输入的初始数据下x.shape:\",x.shape)\n","        x = self.patch_to_embedding(x) #[b,n,dim]\n","        #print(\"patch_to_embedding处理之后的x.shape:\",x.shape)\n","        b, n, _ = x.shape\n","        #print(\"b:\",b,\" | n:\",n)\n","\n","        # add position embedding\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b) #[b,1,dim]\n","        x = torch.cat((cls_tokens, x), dim = 1) #[b,n+1,dim]\n","        #print(\"cls_tokens处理之后的x.shape:\",x.shape)\n","        x += self.pos_embedding[:, :(n + 1)]\n","        #print(\"pos_embedding处理之后的x.shape:\",x.shape)\n","        x = self.dropout(x)\n","\n","        # transformer: x[b,n + 1,dim] -> x[b,n + 1,dim]\n","        x = self.transformer(x, mask)\n","        #print(\"transformer处理之后的x.shape:\",x.shape)\n","\n","        # classification: using cls_token output\n","        x = self.to_latent(x[:,0])\n","\n","        # MLP classification layer\n","        return self.mlp_head(x)"]},{"cell_type":"code","source":["import argparse\n","parser = argparse.ArgumentParser(\"HSI\")\n","parser.add_argument('--dataset', choices=['Indian', 'Pavia', 'Houston'], default='Indian', help='dataset to use')\n","parser.add_argument('--flag_test', choices=['train', 'train'], default='test', help='testing mark')\n","parser.add_argument('--mode', choices=['ViT', 'CAF'], default='CAF', help='mode choice')\n","parser.add_argument('--gpu_id', default='0', help='gpu id')\n","parser.add_argument('--seed', type=int, default=0, help='number of seed')\n","parser.add_argument('--batch_size', type=int, default=15, help='number of batch size')\n","parser.add_argument('--test_freq', type=int, default=5, help='number of evaluation')\n","parser.add_argument('--patches', type=int, default=15, help='number of patches')\n","parser.add_argument('--band_patches', type=int, default=1, help='number of related band')\n","parser.add_argument('--epoches', type=int, default=1000, help='epoch number')\n","parser.add_argument('--learning_rate', type=float, default=8e-4, help='learning rate')\n","parser.add_argument('--gamma', type=float, default=0.9, help='gamma')\n","parser.add_argument('--weight_decay', type=float, default=0, help='weight_decay')\n","parser.add_argument('-f')\n","args = parser.parse_args()"],"metadata":{"id":"IYZ5aTbBNYr0","executionInfo":{"status":"ok","timestamp":1680940813837,"user_tz":-480,"elapsed":835,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["x1 = torch.randn(1, 1, 30, 15, 15)\n","net = ViT(\n","    image_size = args.patches,\n","    near_band = args.band_patches,\n","    num_patches = 30,\n","    num_classes = 16,\n","    dim = 64,\n","    depth = 5,\n","    heads = 4,\n","    mlp_dim = 8,\n","    dropout = 0.1,\n","    emb_dropout = 0.1,\n","    mode = args.mode\n",")\n","y1 = net(x1)\n","print(\"******************* model has down *****************\")\n","print(y1.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNyBufisM-C9","executionInfo":{"status":"ok","timestamp":1680940856161,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"d4a20ff5-03b1-4f04-fd1d-afd6cdf24225"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["******************* model has down *****************\n","torch.Size([1, 16])\n"]}]},{"cell_type":"code","source":["import torch\n","import argparse\n","import torch.nn as nn\n","import torch.utils.data as Data\n","import torch.backends.cudnn as cudnn\n","from scipy.io import loadmat\n","from scipy.io import savemat\n","from torch import optim\n","from torch.autograd import Variable\n","#from vit_pytorch import ViT\n","from sklearn.metrics import confusion_matrix\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","import numpy as np\n","import time\n","import os"],"metadata":{"id":"qgUsPhSMOs93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import argparse\n","import torch.nn as nn\n","import torch.utils.data as Data\n","import torch.backends.cudnn as cudnn\n","from scipy.io import loadmat\n","from scipy.io import savemat\n","from torch import optim\n","from torch.autograd import Variable\n","#from vit_pytorch import ViT\n","from sklearn.metrics import confusion_matrix\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","import numpy as np\n","import time\n","import os\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","parser = argparse.ArgumentParser(\"HSI\")\n","parser.add_argument('--dataset', choices=['Indian', 'Pavia', 'Houston'], default='Indian', help='dataset to use')\n","parser.add_argument('--flag_test', choices=['train', 'train'], default='test', help='testing mark')\n","parser.add_argument('--mode', choices=['ViT', 'CAF'], default='ViT', help='mode choice')\n","parser.add_argument('--gpu_id', default='0', help='gpu id')\n","parser.add_argument('--seed', type=int, default=0, help='number of seed')\n","parser.add_argument('--batch_size', type=int, default=16, help='number of batch size')\n","parser.add_argument('--test_freq', type=int, default=5, help='number of evaluation')\n","parser.add_argument('--patches', type=int, default=1, help='number of patches')\n","parser.add_argument('--band_patches', type=int, default=1, help='number of related band')\n","parser.add_argument('--epoches', type=int, default=1000, help='epoch number')\n","parser.add_argument('--learning_rate', type=float, default=8e-4, help='learning rate')\n","parser.add_argument('--gamma', type=float, default=0.9, help='gamma')\n","parser.add_argument('--weight_decay', type=float, default=0, help='weight_decay')\n","parser.add_argument('-f')\n","args = parser.parse_args()\n","\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n","#-------------------------------------------------------------------------------\n","# 定位训练和测试样本\n","def chooose_train_and_test_point(train_data, test_data, true_data, num_classes):\n","    number_train = []\n","    pos_train = {}\n","    number_test = []\n","    pos_test = {}\n","    number_true = []\n","    pos_true = {}\n","    #-------------------------for train data------------------------------------\n","    for i in range(num_classes):\n","        each_class = []\n","        each_class = np.argwhere(train_data==(i+1))#返回所有训练点索引\n","        number_train.append(each_class.shape[0])#训练点数目\n","        pos_train[i] = each_class#将训练位置索引赋值到pos_train字典中\n","    #print(\"pos_train][1](看看能不能输出标签为2的点位：)\",pos_train[1])#可以\n","    total_pos_train = pos_train[0]#将标签值为1的点位位置赋予total_pos_train\n","    print(total_pos_train.shape)#50\n","    for i in range(1, num_classes):\n","        total_pos_train = np.r_[total_pos_train, pos_train[i]] #(695,2)。np.r_是按列连接两个矩阵，就是把两矩阵上下相加，要求列数相等\n","    total_pos_train = total_pos_train.astype(int)\n","    #--------------------------for test data------------------------------------\n","    for i in range(num_classes):\n","        each_class = []\n","        each_class = np.argwhere(test_data==(i+1))\n","        number_test.append(each_class.shape[0])\n","        pos_test[i] = each_class\n","\n","    total_pos_test = pos_test[0]\n","    for i in range(1, num_classes):\n","        total_pos_test = np.r_[total_pos_test, pos_test[i]] #(9671,2)\n","    total_pos_test = total_pos_test.astype(int)\n","    #--------------------------for true data------------------------------------\n","    for i in range(num_classes+1):\n","        each_class = []\n","        each_class = np.argwhere(true_data==i)\n","        number_true.append(each_class.shape[0])\n","        pos_true[i] = each_class\n","\n","    total_pos_true = pos_true[0]\n","    for i in range(1, num_classes+1):\n","        total_pos_true = np.r_[total_pos_true, pos_true[i]]\n","    total_pos_true = total_pos_true.astype(int)\n","    print(\"**************************************************\")\n","    #total_pos_train训练数据的点位位置矩阵，number_train训练数据中每一类目标的数据列表\n","    print(\"total_pos_train.shape:\",total_pos_train.shape,\"| total_pos_train.type:\",type(total_pos_train))\n","    print(\"number_train:\",number_train,\"| number_train.type:\",type(number_train))\n","    print(\"**************************************************\")\n","\n","    return total_pos_train, total_pos_test, total_pos_true, number_train, number_test, number_true\n","#-------------------------------------------------------------------------------\n","# 边界拓展：镜像\n","def mirror_hsi(height,width,band,input_normalize,patch=5):\n","    padding=patch//2\n","    mirror_hsi=np.zeros((height+2*padding,width+2*padding,band),dtype=float)\n","    #中心区域\n","    mirror_hsi[padding:(padding+height),padding:(padding+width),:]=input_normalize\n","    #左边镜像\n","    for i in range(padding):\n","        mirror_hsi[padding:(height+padding),i,:]=input_normalize[:,padding-i-1,:]\n","    #右边镜像\n","    for i in range(padding):\n","        mirror_hsi[padding:(height+padding),width+padding+i,:]=input_normalize[:,width-1-i,:]\n","    #上边镜像\n","    for i in range(padding):\n","        mirror_hsi[i,:,:]=mirror_hsi[padding*2-i-1,:,:]\n","    #下边镜像\n","    for i in range(padding):\n","        mirror_hsi[height+padding+i,:,:]=mirror_hsi[height+padding-1-i,:,:]\n","\n","    print(\"**************************************************\")\n","    print(\"patch is : {}\".format(patch))\n","    print(\"mirror_image shape : [{0},{1},{2}]\".format(mirror_hsi.shape[0],mirror_hsi.shape[1],mirror_hsi.shape[2]))\n","    print(\"**************************************************\")\n","    return mirror_hsi\n","#-------------------------------------------------------------------------------\n","# 获取patch的图像数据\n","def gain_neighborhood_pixel(mirror_image, point, i, patch=5):\n","    x = point[i,0]\n","    y = point[i,1]\n","    temp_image = mirror_image[x:(x+patch),y:(y+patch),:]\n","    return temp_image\n","\n","def gain_neighborhood_band(x_train, band, band_patch, patch=5):\n","    nn = band_patch // 2\n","    pp = (patch*patch) // 2\n","    x_train_reshape = x_train.reshape(x_train.shape[0], patch*patch, band)\n","    x_train_band = np.zeros((x_train.shape[0], patch*patch*band_patch, band),dtype=float)\n","    # 中心区域\n","    x_train_band[:,nn*patch*patch:(nn+1)*patch*patch,:] = x_train_reshape\n","    #左边镜像\n","    for i in range(nn):\n","        if pp > 0:\n","            x_train_band[:,i*patch*patch:(i+1)*patch*patch,:i+1] = x_train_reshape[:,:,band-i-1:]\n","            x_train_band[:,i*patch*patch:(i+1)*patch*patch,i+1:] = x_train_reshape[:,:,:band-i-1]\n","        else:\n","            x_train_band[:,i:(i+1),:(nn-i)] = x_train_reshape[:,0:1,(band-nn+i):]\n","            x_train_band[:,i:(i+1),(nn-i):] = x_train_reshape[:,0:1,:(band-nn+i)]\n","    #右边镜像\n","    for i in range(nn):\n","        if pp > 0:\n","            x_train_band[:,(nn+i+1)*patch*patch:(nn+i+2)*patch*patch,:band-i-1] = x_train_reshape[:,:,i+1:]\n","            x_train_band[:,(nn+i+1)*patch*patch:(nn+i+2)*patch*patch,band-i-1:] = x_train_reshape[:,:,:i+1]\n","        else:\n","            x_train_band[:,(nn+1+i):(nn+2+i),(band-i-1):] = x_train_reshape[:,0:1,:(i+1)]\n","            x_train_band[:,(nn+1+i):(nn+2+i),:(band-i-1)] = x_train_reshape[:,0:1,(i+1):]\n","    return x_train_band\n","#-------------------------------------------------------------------------------\n","# 汇总训练数据和测试数据\n","def train_and_test_data(mirror_image, band, train_point, test_point, true_point, patch=5, band_patch=3):\n","    print(\"train_point.shape[0]:\",train_point.shape[0])\n","    x_train = np.zeros((train_point.shape[0], patch, patch, band), dtype=float)\n","    print(\"x_train.shape:\",x_train.shape)\n","    x_test = np.zeros((test_point.shape[0], patch, patch, band), dtype=float)\n","    x_true = np.zeros((true_point.shape[0], patch, patch, band), dtype=float)\n","    for i in range(train_point.shape[0]):#(695)\n","        x_train[i,:,:,:] = gain_neighborhood_pixel(mirror_image, train_point, i, patch)\n","    for j in range(test_point.shape[0]):\n","        x_test[j,:,:,:] = gain_neighborhood_pixel(mirror_image, test_point, j, patch)\n","    for k in range(true_point.shape[0]):\n","        x_true[k,:,:,:] = gain_neighborhood_pixel(mirror_image, true_point, k, patch)\n","    print(\"x_train shape = {}, type = {}\".format(x_train.shape,x_train.dtype))\n","    print(\"x_test  shape = {}, type = {}\".format(x_test.shape,x_test.dtype))\n","    print(\"x_true  shape = {}, type = {}\".format(x_true.shape,x_test.dtype))\n","    print(\"**************************************************\")\n","    \n","    x_train_band = gain_neighborhood_band(x_train, band, band_patch, patch)\n","    x_test_band = gain_neighborhood_band(x_test, band, band_patch, patch)\n","    x_true_band = gain_neighborhood_band(x_true, band, band_patch, patch)\n","    print(\"x_train_band shape = {}, type = {}\".format(x_train_band.shape,x_train_band.dtype))\n","    print(\"x_test_band  shape = {}, type = {}\".format(x_test_band.shape,x_test_band.dtype))\n","    print(\"x_true_band  shape = {}, type = {}\".format(x_true_band.shape,x_true_band.dtype))\n","    print(\"**************************************************\")\n","    return x_train_band, x_test_band, x_true_band\n","#-------------------------------------------------------------------------------\n","# 标签y_train, y_test\n","def train_and_test_label(number_train, number_test, number_true, num_classes):\n","    y_train = []\n","    y_test = []\n","    y_true = []\n","    for i in range(num_classes):\n","        for j in range(number_train[i]):\n","            y_train.append(i)\n","        for k in range(number_test[i]):\n","            y_test.append(i)\n","    for i in range(num_classes+1):\n","        for j in range(number_true[i]):\n","            y_true.append(i)\n","    y_train = np.array(y_train)\n","    y_test = np.array(y_test)\n","    y_true = np.array(y_true)\n","    print(\"y_train: shape = {} ,type = {}\".format(y_train.shape,y_train.dtype))\n","    print(\"y_test: shape = {} ,type = {}\".format(y_test.shape,y_test.dtype))\n","    print(\"y_true: shape = {} ,type = {}\".format(y_true.shape,y_true.dtype))\n","    print(\"**************************************************\")\n","    return y_train, y_test, y_true\n","#-------------------------------------------------------------------------------\n","class AvgrageMeter(object):\n","\n","  def __init__(self):\n","    self.reset()\n","\n","  def reset(self):\n","    self.avg = 0\n","    self.sum = 0\n","    self.cnt = 0\n","\n","  def update(self, val, n=1):\n","    self.sum += val * n\n","    self.cnt += n\n","    self.avg = self.sum / self.cnt\n","#-------------------------------------------------------------------------------\n","def accuracy(output, target, topk=(1,)):\n","  maxk = max(topk)\n","  batch_size = target.size(0)\n","\n","  _, pred = output.topk(maxk, 1, True, True)\n","  pred = pred.t()\n","  correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","  res = []\n","  for k in topk:\n","    correct_k = correct[:k].view(-1).float().sum(0)\n","    res.append(correct_k.mul_(100.0/batch_size))\n","  return res, target, pred.squeeze()\n","#-------------------------------------------------------------------------------\n","# train model\n","def train_epoch(model, train_loader, criterion, optimizer):\n","    objs = AvgrageMeter()\n","    top1 = AvgrageMeter()\n","    tar = np.array([])\n","    pre = np.array([])\n","    for batch_idx, (batch_data, batch_target) in enumerate(train_loader):\n","        batch_data = batch_data.to(device)\n","        batch_target = batch_target.to(device) \n","\n","        optimizer.zero_grad()\n","        batch_pred = model(batch_data,mask = None)\n","        loss = criterion(batch_pred, batch_target)\n","        loss.backward()\n","        optimizer.step()\n","      \n","\n","        prec1, t, p = accuracy(batch_pred, batch_target, topk=(1,))\n","        n = batch_data.shape[0]\n","        objs.update(loss.data, n)\n","        top1.update(prec1[0].data, n)\n","        tar = np.append(tar, t.data.cpu().numpy())\n","        pre = np.append(pre, p.data.cpu().numpy())\n","    return top1.avg, objs.avg, tar, pre\n","#-------------------------------------------------------------------------------\n","# validate model\n","def valid_epoch(model, valid_loader, criterion, optimizer):\n","    objs = AvgrageMeter()\n","    top1 = AvgrageMeter()\n","    tar = np.array([])\n","    pre = np.array([])\n","    for batch_idx, (batch_data, batch_target) in enumerate(valid_loader):\n","        batch_data = batch_data.to(device)\n","        batch_target = batch_target.to(device) \n","\n","        batch_pred = model(batch_data)\n","        \n","        loss = criterion(batch_pred, batch_target)\n","\n","        prec1, t, p = accuracy(batch_pred, batch_target, topk=(1,))\n","        n = batch_data.shape[0]\n","        objs.update(loss.data, n)\n","        top1.update(prec1[0].data, n)\n","        tar = np.append(tar, t.data.cpu().numpy())\n","        pre = np.append(pre, p.data.cpu().numpy())\n","        \n","    return tar, pre\n","\n","def test_epoch(model, test_loader, criterion, optimizer):\n","    objs = AvgrageMeter()\n","    top1 = AvgrageMeter()\n","    tar = np.array([])\n","    pre = np.array([])\n","    for batch_idx, (batch_data, batch_target) in enumerate(test_loader):\n","        batch_data = batch_data.to(device)\n","        batch_target = batch_target.to(device) \n","\n","        batch_pred = model(batch_data)\n","\n","        _, pred = batch_pred.topk(1, 1, True, True)\n","        pp = pred.squeeze()\n","        pre = np.append(pre, pp.data.cpu().numpy())\n","    return pre\n","#-------------------------------------------------------------------------------\n","def output_metric(tar, pre):\n","    matrix = confusion_matrix(tar, pre)\n","    OA, AA_mean, Kappa, AA = cal_results(matrix)\n","    return OA, AA_mean, Kappa, AA\n","#-------------------------------------------------------------------------------\n","def cal_results(matrix):\n","    shape = np.shape(matrix)\n","    number = 0\n","    sum = 0\n","    AA = np.zeros([shape[0]], dtype=float)\n","    for i in range(shape[0]):\n","        number += matrix[i, i]\n","        AA[i] = matrix[i, i] / np.sum(matrix[i, :])\n","        sum += np.sum(matrix[i, :]) * np.sum(matrix[:, i])\n","    OA = number / np.sum(matrix)\n","    AA_mean = np.mean(AA)\n","    pe = sum / (np.sum(matrix) ** 2)\n","    Kappa = (OA - pe) / (1 - pe)\n","    return OA, AA_mean, Kappa, AA\n","#-------------------------------------------------------------------------------\n","# Parameter Setting\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.cuda.manual_seed(args.seed)\n","cudnn.deterministic = True\n","cudnn.benchmark = False\n","# prepare data\n","if args.dataset == 'Indian':\n","    data = loadmat('/content/drive/MyDrive/AI data/SpectralFormer/IndianPine.mat')\n","elif args.dataset == 'Pavia':\n","    data = loadmat('/content/drive/MyDrive/AI data/hyperspetral image/PaviaU.mat')\n","#elif args.dataset == 'Houston':\n","#    data = loadmat('./data/Houston.mat')\n","else:\n","    raise ValueError(\"Unkknow dataset\")\n","color_mat = loadmat('/content/drive/MyDrive/AI data/SpectralFormer/AVIRIS_colormap.mat')\n","TR = data['TR']\n","print(\"TR.shape\",TR.shape)\n","\n","TE = data['TE']\n","print(\"TE.shape\",TE.shape)\n","\n","input = data['input'] #(145,145,200)\n","label = TR + TE\n","print(\"label:\",label.shape)\n","num_classes = np.max(TR)\n","print(\"num_classes:\",num_classes)\n","\n","color_mat_list = list(color_mat)\n","print(\"color_mat_list是什么？\")\n","print(color_mat_list)\n","color_matrix = color_mat[color_mat_list[3]] #(17,3)好像是用于显示图像中的颜色处理的？？？？？？？？？？？？？？\n","print(\"color_matrix.shape:\",color_matrix.shape)\n","# normalize data by band norm\n","input_normalize = np.zeros(input.shape)\n","print(\"input_normalize:\",input_normalize.shape)\n","for i in range(input.shape[2]):\n","    input_max = np.max(input[:,:,i])\n","    input_min = np.min(input[:,:,i])\n","    input_normalize[:,:,i] = (input[:,:,i]-input_min)/(input_max-input_min)\n","# data size\n","height, width, band = input.shape\n","print(\"height={0},width={1},band={2}\".format(height, width, band))\n","#-------------------------------------------------------------------------------\n","# obtain train and test data\n","total_pos_train, total_pos_test, total_pos_true, number_train, number_test, number_true = chooose_train_and_test_point(TR, TE, label, num_classes)\n","# 定位训练和测试样本\n","#total_pos_train训练数据的点位位置矩阵，number_train训练数据中每一类目标的数据列表。\n","#total_pos_test，total_pos_truenumber_test, number_true同理\n","mirror_image = mirror_hsi(height, width, band, input_normalize, patch=args.patches)\n","print(\"input_normalize.shape:\",input_normalize.shape)\n","print(\"mirror_image.shape:\",mirror_image.shape)\n","print(\"_____________________# 汇总训练数据和测试数据_____________________\")\n","x_train_band, x_test_band, x_true_band = train_and_test_data(mirror_image, band, total_pos_train, total_pos_test, total_pos_true, patch=args.patches, band_patch=args.band_patches)\n","y_train, y_test, y_true = train_and_test_label(number_train, number_test, number_true, num_classes)\n","#-------------------------------------------------------------------------------\n","# load data\n","x_train=torch.from_numpy(x_train_band.transpose(0,2,1)).type(torch.FloatTensor) #[695, 200, 7, 7]\n","y_train=torch.from_numpy(y_train).type(torch.LongTensor) #[695]\n","Label_train=Data.TensorDataset(x_train,y_train)\n","x_test=torch.from_numpy(x_test_band.transpose(0,2,1)).type(torch.FloatTensor) # [9671, 200, 7, 7]\n","y_test=torch.from_numpy(y_test).type(torch.LongTensor) # [9671]\n","Label_test=Data.TensorDataset(x_test,y_test)\n","x_true=torch.from_numpy(x_true_band.transpose(0,2,1)).type(torch.FloatTensor)\n","y_true=torch.from_numpy(y_true).type(torch.LongTensor)\n","Label_true=Data.TensorDataset(x_true,y_true)\n","print(\"__________________________loda data__________________________\")\n","print(\"**************************************************************\")\n","print(\"x_train.shape:\",x_train.shape)\n","print(\"y_train.shape:\",y_train.shape)\n","#print(\"Label_train.shape\",Label_train.shape)\n","print(\"x_test.shape:\",x_test.shape)\n","print(\"y_test.shape:\",y_test.shape)\n","#print(\"Label_test.shape:\",Label_test.shape)\n","print(\"x_true.shape:\",x_true.shape)\n","print(\"y_true.shape:\",y_true.shape)\n","#print(\"Label_true.shape:\",Label_true.shape)\n","print(\"**************************************************************\")\n","\n","label_train_loader=Data.DataLoader(Label_train,batch_size=args.batch_size,shuffle=True)\n","label_test_loader=Data.DataLoader(Label_test,batch_size=args.batch_size,shuffle=True)\n","label_true_loader=Data.DataLoader(Label_true,batch_size=100,shuffle=False)\n","\n","#-------------------------------------------------------------------------------\n","# create model\n","model = ViT(\n","    image_size = args.patches,\n","    near_band = args.band_patches,\n","    num_patches = band,\n","    num_classes = num_classes,\n","    dim = 64,\n","    depth = 5,\n","    heads = 4,\n","    mlp_dim = 8,\n","    dropout = 0.1,\n","    emb_dropout = 0.1,\n","    mode = args.mode\n",")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","# criterion\n","criterion = nn.CrossEntropyLoss().to(device)\n","# optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.epoches//10, gamma=args.gamma)\n","#-------------------------------------------------------------------------------\n","if args.flag_test == 'test':\n","    if args.mode == 'ViT':\n","        model=torch.load('/content/model_995.pth',map_location=torch.device(\"cuda:0\"))     \n","    elif (args.mode == 'CAF') & (args.patches == 1):\n","        model.load_state_dict(torch.load('./SpectralFormer_pixel.pt'))\n","    elif (args.mode == 'CAF') & (args.patches == 7):\n","        model.load_state_dict(torch.load('./SpectralFormer_patch.pt'))\n","    else:\n","        raise ValueError(\"Wrong Parameters\") \n","    model.eval()\n","    tar_v, pre_v = valid_epoch(model, label_test_loader, criterion, optimizer)\n","    OA2, AA_mean2, Kappa2, AA2 = output_metric(tar_v, pre_v)\n","\n","    # output classification maps\n","    pre_u = test_epoch(model, label_true_loader, criterion, optimizer)\n","    prediction_matrix = np.zeros((height, width), dtype=float)\n","    for i in range(total_pos_true.shape[0]):\n","        prediction_matrix[total_pos_true[i,0], total_pos_true[i,1]] = pre_u[i] + 1\n","    plt.subplot(1,1,1)\n","    plt.imshow(prediction_matrix, colors.ListedColormap(color_matrix))\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.show()\n","    savemat('matrix.mat',{'P':prediction_matrix, 'label':label})\n","elif args.flag_test == 'train':\n","    print(\"start training\")\n","    tic = time.time()\n","    for epoch in range(args.epoches): \n","        scheduler.step()\n","\n","        # train model\n","        model.train()\n","        train_acc, train_obj, tar_t, pre_t = train_epoch(model, label_train_loader, criterion, optimizer)\n","        OA1, AA_mean1, Kappa1, AA1 = output_metric(tar_t, pre_t) \n","        print(\"Epoch: {:03d} train_loss: {:.4f} train_acc: {:.4f}\"\n","                        .format(epoch+1, train_obj, train_acc))\n","        \n","\n","        if (epoch % args.test_freq == 0) | (epoch == args.epoches - 1):         \n","            model.eval()\n","            tar_v, pre_v = valid_epoch(model, label_test_loader, criterion, optimizer)\n","            OA2, AA_mean2, Kappa2, AA2 = output_metric(tar_v, pre_v)\n","        if epoch > 990:\n","          torch.save(model,\"model_{}.pth\".format(epoch))\n","          print(\"................第{}轮迭代模型已保存...............\".format(epoch))\n","    toc = time.time()\n","    print(\"Running Time: {:.2f}\".format(toc-tic))\n","    \n","    print(\"**************************************************\")\n","\n","print(\"Final result:\")\n","print(\"OA: {:.4f} | AA: {:.4f} | Kappa: {:.4f}\".format(OA2, AA_mean2, Kappa2))\n","print(AA2)\n","print(\"**************************************************\")\n","print(\"Parameter:\")\n","\n","def print_args(args):\n","    for k, v in zip(args.keys(), args.values()):\n","        print(\"{0}: {1}\".format(k,v))\n","\n","print_args(vars(args))"],"metadata":{"id":"Q3aOVttV39ZQ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1669200123412,"user_tz":-480,"elapsed":1911,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"10ef043d-1ef6-4745-eb06-b2574392dd0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TR.shape (145, 145)\n","TE.shape (145, 145)\n","label: (145, 145)\n","num_classes: 16\n","color_mat_list是什么？\n","['__header__', '__version__', '__globals__', 'mycolormap']\n","color_matrix.shape: (17, 3)\n","input_normalize: (145, 145, 200)\n","height=145,width=145,band=200\n","(50, 2)\n","**************************************************\n","total_pos_train.shape: (695, 2) | total_pos_train.type: <class 'numpy.ndarray'>\n","number_train: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 15, 15, 15] | number_train.type: <class 'list'>\n","**************************************************\n","**************************************************\n","patch is : 1\n","mirror_image shape : [145,145,200]\n","**************************************************\n","input_normalize.shape: (145, 145, 200)\n","mirror_image.shape: (145, 145, 200)\n","_____________________# 汇总训练数据和测试数据_____________________\n","train_point.shape[0]: 695\n","x_train.shape: (695, 1, 1, 200)\n","x_train shape = (695, 1, 1, 200), type = float64\n","x_test  shape = (9671, 1, 1, 200), type = float64\n","x_true  shape = (21025, 1, 1, 200), type = float64\n","**************************************************\n","x_train_band shape = (695, 1, 200), type = float64\n","x_test_band  shape = (9671, 1, 200), type = float64\n","x_true_band  shape = (21025, 1, 200), type = float64\n","**************************************************\n","y_train: shape = (695,) ,type = int64\n","y_test: shape = (9671,) ,type = int64\n","y_true: shape = (21025,) ,type = int64\n","**************************************************\n","__________________________loda data__________________________\n","**************************************************************\n","x_train.shape: torch.Size([695, 200, 1])\n","y_train.shape: torch.Size([695])\n","x_test.shape: torch.Size([9671, 200, 1])\n","y_test.shape: torch.Size([9671])\n","x_true.shape: torch.Size([21025, 200, 1])\n","y_true.shape: torch.Size([21025])\n","**************************************************************\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c385b18dfda0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ViT'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model_995.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CAF'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./SpectralFormer_pixel.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/model_995.pth'"]}]},{"cell_type":"code","source":["data.keys()"],"metadata":{"id":"0nf4K6PJh7KY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(input))\n","input1=input[:,:,0:1]\n","print(input1.shape)\n","input1 = input1.reshape(145,145)\n","print(input1.shape)"],"metadata":{"id":"2gG1xjHPPK_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(TE))\n","import xlwt  # 负责写excel\n","import xlrd\n","filename =xlwt.Workbook() #创建工作簿\n","sheet1 = filename.add_sheet(u'sheet1',cell_overwrite_ok=True) #创建sheet\n","[h,l]=TE.shape #h为行数，l为列数\n","for i in range (h):\n","    for j in range (l):\n","      sheet1.write(i,j,int(TE[i,j]))\n","filename.save('name_of_your_excel_fileTE.xls')"],"metadata":{"id":"37xV9-UdMmeE"},"execution_count":null,"outputs":[]}]}