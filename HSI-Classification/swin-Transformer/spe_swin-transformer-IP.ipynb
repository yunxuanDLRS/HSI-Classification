{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1VWR37vdLzcYVdbwURzCuqdEzQVSrAJRQ","authorship_tag":"ABX9TyMdPMkuX52iGyg4sTS8M61H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25SddX5JZUbJ","executionInfo":{"status":"ok","timestamp":1673595108012,"user_tz":-480,"elapsed":5031,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"e820e7f9-c089-42dd-e2a1-4933b9f5756a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spectral\n","  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from spectral) (1.21.6)\n","Installing collected packages: spectral\n","Successfully installed spectral-0.23.1\n"]}],"source":["#! wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n","#! wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n","! pip install spectral"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io as sio\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n","import spectral\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import copy"],"metadata":{"id":"_25rp17_ZjQi","executionInfo":{"status":"ok","timestamp":1673595112392,"user_tz":-480,"elapsed":4385,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","import numpy as np\n","from typing import Optional\n","\n","\n","def drop_path_f(x, drop_prob: float = 0., training: bool = False):\n","    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n","\n","    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n","    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n","    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n","    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n","    'survival rate' as the argument.\n","\n","    \"\"\"\n","    if drop_prob == 0. or not training:\n","        return x\n","    keep_prob = 1 - drop_prob\n","    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n","    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n","    random_tensor.floor_()  # binarize\n","    output = x.div(keep_prob) * random_tensor\n","    return output\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","    def __init__(self, drop_prob=None):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path_f(x, self.drop_prob, self.training)\n","\n","\n","def window_partition(x, window_size: int):\n","    \"\"\"\n","    将feature map按照window_size划分成一个个没有重叠的window\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size(M)\n","\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    # permute: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H//Mh, W//Mh, Mw, Mw, C]\n","    # view: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B*num_windows, Mh, Mw, C]\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size: int, H: int, W: int):\n","    \"\"\"\n","    将一个个window还原成一个feature map\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size(M)\n","        H (int): Height of image\n","        W (int): Width of image\n","\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    # view: [B*num_windows, Mh, Mw, C] -> [B, H//Mh, W//Mw, Mh, Mw, C]\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    # permute: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B, H//Mh, Mh, W//Mw, Mw, C]\n","    # view: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H, W, C]\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\"\n","    2D Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, patch_size=4, in_c=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        patch_size = (patch_size, patch_size)\n","        self.patch_size = patch_size\n","        self.in_chans = in_c\n","        self.embed_dim = embed_dim\n","        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n","\n","    def forward(self, x):\n","        _, _, H, W = x.shape\n","\n","        # padding\n","        # 如果输入图片的H，W不是patch_size的整数倍，需要进行padding\n","        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)\n","        if pad_input:\n","            # to pad the last 3 dimensions,\n","            # (W_left, W_right, H_top,H_bottom, C_front, C_back)\n","            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],\n","                          0, self.patch_size[0] - H % self.patch_size[0],\n","                          0, 0))\n","\n","        # 下采样patch_size倍\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        # flatten: [B, C, H, W] -> [B, C, HW]\n","        # transpose: [B, C, HW] -> [B, HW, C]\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","        return x, H, W\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x, H, W):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        # padding\n","        # 如果输入feature map的H，W不是2的整数倍，需要进行padding\n","        pad_input = (H % 2 == 1) or (W % 2 == 1)\n","        if pad_input:\n","            # to pad the last 3 dimensions, starting from the last dimension and moving forward.\n","            # (C_front, C_back, W_left, W_right, H_top, H_bottom)\n","            # 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同\n","            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n","\n","        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]\n","        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]\n","        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]\n","        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]\n","        x = torch.cat([x0, x1, x2, x3], -1)  # [B, H/2, W/2, 4*C]\n","        x = x.view(B, -1, 4 * C)  # [B, H/2*W/2, 4*C]\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)  # [B, H/2*W/2, 2*C]\n","\n","        return x\n","\n","\n","class Mlp(nn.Module):\n","    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n","    \"\"\"\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.drop1 = nn.Dropout(drop)\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop2 = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop1(x)\n","        x = self.fc2(x)\n","        x = self.drop2(x)\n","        return x\n","\n","\n","class WindowAttention(nn.Module):\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # [Mh, Mw]\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Mh-1 * 2*Mw-1, nH]\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # [2, Mh, Mw]\n","        coords_flatten = torch.flatten(coords, 1)  # [2, Mh*Mw]\n","        # [2, Mh*Mw, 1] - [2, 1, Mh*Mw]\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # [2, Mh*Mw, Mh*Mw]\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # [Mh*Mw, Mh*Mw, 2]\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # [Mh*Mw, Mh*Mw]\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n","\n","    def forward(self, x, mask: Optional[torch.Tensor] = None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, Mh*Mw, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        # [batch_size*num_windows, Mh*Mw, total_embed_dim]\n","        B_, N, C = x.shape\n","        # qkv(): -> [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]\n","        # reshape: -> [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]\n","        # permute: -> [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        # [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #下面的q,k,v的shape如上注释所示，其意义是：\n","        #batch_size*num_windows表示所有数据的总窗口数，就相当于ViT中的batch_size\n","        #num_heads在这里可以不用管\n","        #Mh*Mw表示窗口的长乘宽。意义相当于patch的个数或者长度\n","        #embed_dim_per_head表示dim经过分头后的长度。意义相当于patch的个数或者长度\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #综上所述该网络是将所有图片的所有window看作一个数据整体（相当于ViT中的batch_size），Mh*Mw和embed_dim_per_head看作是每个window的多个具有长度的patch。从而计算每张图片中每个window中的注意力（通俗理解：网络计算的是每个window中内部点位之间的相关性）\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n","\n","        #****************************ChannelAttention**************************\n","        #q_c, k_c, v_c = qkv[0], qkv[1], qkv[2]\n","\n","        #q_c = q_c.transpose(-2, -1)\n","        #k_c = k_c.transpose(-2, -1)\n","        #v_c = v_c.transpose(-2, -1)\n","\n","        #q_c = F.normalize(q_c, dim=-1)\n","        #k_c = F.normalize(k_c, dim=-1)\n","\n","        #attn_c = (q_c @ k_c.transpose(-2, -1)) * self.temperature\n","        #attn_c = attn_c.softmax(dim=-1)\n","        #attn_c = self.attn_drop(attn_c)\n","        #x_c = (attn_c @ v_c).permute(0, 3, 1, 2).reshape(B_, N, C)\n","        #x_c = self.proj(x_c)\n","        #x_c = self.proj_drop(x_c)\n","        #****************************ChannelAttention had down**************************\n","\n","        # transpose: -> [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]\n","        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        # relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -> [Mh*Mw,Mh*Mw,nH]\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # [nH, Mh*Mw, Mh*Mw]\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            # mask: [nW, Mh*Mw, Mh*Mw]\n","            nW = mask.shape[0]  # num_windows\n","            # attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]\n","            # mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n","        # transpose: -> [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]\n","        # reshape: -> [batch_size*num_windows, Mh*Mw, total_embed_dim]\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        #x = x + x_c\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        #x = x + x_c\n","        \n","        return x\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    r\"\"\" Swin Transformer Block.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n","                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,\n","            attn_drop=attn_drop, proj_drop=drop)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","    def forward(self, x, attn_mask):\n","        H, W = self.H, self.W\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # pad feature maps to multiples of window size\n","        # 把feature map给pad到window size的整数倍\n","        pad_l = pad_t = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = x.shape\n","\n","        # cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","\n","        # partition windows\n","        x_windows = window_partition(shifted_x, self.window_size)  # [nW*B, Mh, Mw, C]\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # [nW*B, Mh*Mw, C]\n","\n","        # W-MSA/SW-MSA\n","        attn_windows = self.attn(x_windows, mask=attn_mask)  # [nW*B, Mh*Mw, C]\n","\n","        # merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)  # [nW*B, Mh, Mw, C]\n","        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # [B, H', W', C]\n","\n","        # reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if pad_r > 0 or pad_b > 0:\n","            # 把前面pad的数据移除掉\n","            x = x[:, :H, :W, :].contiguous()\n","\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\"\n","    A basic Swin Transformer layer for one stage.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","    \"\"\"\n","\n","    def __init__(self, dim, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n","        super().__init__()\n","        self.dim = dim\n","        self.depth = depth\n","        self.window_size = window_size\n","        self.use_checkpoint = use_checkpoint\n","        self.shift_size = window_size // 2\n","\n","        # build blocks\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(\n","                dim=dim,\n","                num_heads=num_heads,\n","                window_size=window_size,\n","                shift_size=0 if (i % 2 == 0) else self.shift_size,\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                drop=drop,\n","                attn_drop=attn_drop,\n","                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                norm_layer=norm_layer)\n","            for i in range(depth)])\n","\n","        # patch merging layer\n","        if downsample is not None:\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def create_mask(self, x, H, W):\n","        # calculate attention mask for SW-MSA\n","        # 保证Hp和Wp是window_size的整数倍\n","        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n","        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n","        # 拥有和feature map一样的通道排列顺序，方便后续window_partition\n","        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # [1, Hp, Wp, 1]\n","        h_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        w_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)  # [nW, Mh, Mw, 1]\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)  # [nW, Mh*Mw]\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]\n","        # [nW, Mh*Mw, Mh*Mw]\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","        return attn_mask\n","\n","    def forward(self, x, H, W):\n","        attn_mask = self.create_mask(x, H, W)  # [nW, Mh*Mw, Mh*Mw]\n","        for blk in self.blocks:\n","            blk.H, blk.W = H, W\n","            if not torch.jit.is_scripting() and self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x, attn_mask)\n","            else:\n","                x = blk(x, attn_mask)\n","        if self.downsample is not None:\n","            x = self.downsample(x, H, W)\n","            H, W = (H + 1) // 2, (W + 1) // 2\n","\n","        return x, H, W\n","\n","\n","class SwinTransformer(nn.Module):\n","    r\"\"\" Swin Transformer\n","        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n","          https://arxiv.org/pdf/2103.14030\n","\n","    Args:\n","        patch_size (int | tuple(int)): Patch size. Default: 4\n","        in_chans (int): Number of input image channels. Default: 3\n","        num_classes (int): Number of classes for classification head. Default: 1000\n","        embed_dim (int): Patch embedding dimension. Default: 96\n","        depths (tuple(int)): Depth of each Swin Transformer layer.\n","        num_heads (tuple(int)): Number of attention heads in different layers.\n","        window_size (int): Window size. Default: 7\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n","        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n","        drop_rate (float): Dropout rate. Default: 0\n","        attn_drop_rate (float): Attention dropout rate. Default: 0\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n","        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n","        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n","    \"\"\"\n","\n","    def __init__(self, patch_size=1, in_chans=30, num_classes=16,\n","                 embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24),\n","                 window_size=7, mlp_ratio=4., qkv_bias=True,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n","                 norm_layer=nn.LayerNorm, patch_norm=True,\n","                 use_checkpoint=False, **kwargs):\n","        super().__init__()\n","\n","        self.num_classes = num_classes\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.patch_norm = patch_norm\n","        # stage4输出特征矩阵的channels\n","        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","        self.mlp_ratio = mlp_ratio\n","\n","        # split image into non-overlapping patches\n","        self.patch_embed = PatchEmbed(\n","            patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","\n","        # build layers\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            # 注意这里构建的stage和论文图中有些差异\n","            # 这里的stage不包含该stage的patch_merging层，包含的是下个stage的\n","            layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n","                                depth=depths[i_layer],\n","                                num_heads=num_heads[i_layer],\n","                                window_size=window_size,\n","                                mlp_ratio=self.mlp_ratio,\n","                                qkv_bias=qkv_bias,\n","                                drop=drop_rate,\n","                                attn_drop=attn_drop_rate,\n","                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n","                                norm_layer=norm_layer,\n","                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n","                                use_checkpoint=use_checkpoint)\n","            self.layers.append(layers)\n","\n","        self.norm = norm_layer(self.num_features)\n","        self.avgpool = nn.AdaptiveAvgPool1d(1)\n","        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, x):\n","        # x: [B, L, C]\n","        x, H, W = self.patch_embed(x)\n","        x = self.pos_drop(x)\n","\n","        for layer in self.layers:\n","            x, H, W = layer(x, H, W)\n","\n","        x = self.norm(x)  # [B, L, C]\n","        x = self.avgpool(x.transpose(1, 2))  # [B, C, 1]\n","        x = torch.flatten(x, 1)\n","        x = self.head(x)\n","        return x\n","\n","\n"],"metadata":{"id":"0BNzXpDZZnL-","executionInfo":{"status":"ok","timestamp":1673595112393,"user_tz":-480,"elapsed":11,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(4, 30, 21, 21)\n","net = SwinTransformer()\n","y = net(x)\n","print(y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yP82zXhHZ7QV","executionInfo":{"status":"ok","timestamp":1673595113995,"user_tz":-480,"elapsed":1612,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"7139e49a-eaa5-4aa9-9461-feee45a15023"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 16])\n"]}]},{"cell_type":"code","source":["# 对高光谱数据 X 应用 PCA 变换\n","def applyPCA(X, numComponents):\n","    newX = np.reshape(X, (-1, X.shape[2]))\n","    pca = PCA(n_components=numComponents, whiten=True)\n","    newX = pca.fit_transform(newX)\n","    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n","    return newX\n","\n","# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\n","def padWithZeros(X, margin=2):\n","    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n","    x_offset = margin\n","    y_offset = margin\n","    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n","    return newX\n","\n","# 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\n","def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n","    # 给 X 做 padding\n","    margin = int((windowSize - 1) / 2)\n","    zeroPaddedX = padWithZeros(X, margin=margin)\n","    # split patches\n","    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n","    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n","    patchIndex = 0\n","    for r in range(margin, zeroPaddedX.shape[0] - margin):\n","        for c in range(margin, zeroPaddedX.shape[1] - margin):\n","            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n","            patchesData[patchIndex, :, :, :] = patch\n","            patchesLabels[patchIndex] = y[r-margin, c-margin]\n","            patchIndex = patchIndex + 1\n","    if removeZeroLabels:\n","        patchesData = patchesData[patchesLabels>0,:,:,:]\n","        patchesLabels = patchesLabels[patchesLabels>0]\n","        patchesLabels -= 1\n","    return patchesData, patchesLabels\n","\n","def splitTrainTestSet(X, y, testRatio, randomState=345):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n","    return X_train, X_test, y_train, y_test"],"metadata":{"id":"t_57IBFvaKDu","executionInfo":{"status":"ok","timestamp":1673595113995,"user_tz":-480,"elapsed":2,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 地物类别\n","class_num = 16\n","X = sio.loadmat('/content/drive/MyDrive/AI data/hyperspetral image/Indian_pines_corrected.mat')['indian_pines_corrected']\n","y = sio.loadmat('/content/drive/MyDrive/AI data/hyperspetral image/Indian_pines_gt.mat')['indian_pines_gt']\n","\n","# 用于测试样本的比例\n","test_ratio = 0.9\n","# 每个像素周围提取 patch 的尺寸\n","patch_size = 21\n","# 使用 PCA 降维，得到主成分的数量\n","pca_components = 30\n","\n","print('Hyperspectral data shape: ', X.shape)\n","print('Label shape: ', y.shape)\n","\n","print('\\n... ... PCA tranformation ... ...')\n","X_pca = applyPCA(X, numComponents=pca_components)\n","print('Data shape after PCA: ', X_pca.shape)\n","\n","print('\\n... ... create data cubes ... ...')\n","X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)\n","print('Data cube X shape: ', X_pca.shape)\n","print('Data cube y shape: ', y.shape)\n","\n","print('\\n... ... create train & test data ... ...')\n","Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X_pca, y, test_ratio)\n","print('Xtrain shape: ', Xtrain.shape)\n","print('Xtest  shape: ', Xtest.shape)\n","\n","# 改变 Xtrain, Ytrain 的形状，以符合 keras 的要求\n","#Xtrain = Xtrain.reshape(-1, patch_size, patch_size, pca_components, 1)\n","#Xtest  = Xtest.reshape(-1, patch_size, patch_size, pca_components, 1)\n","#print('before transpose: Xtrain shape: ', Xtrain.shape) \n","#print('before transpose: Xtest  shape: ', Xtest.shape)\n","\n","\n","# 为了适应 pytorch 结构，数据要做 transpose\n","Xtrain = Xtrain.transpose(0, 3, 1, 2)\n","Xtest  = Xtest.transpose(0, 3, 1, 2)\n","print('after transpose: Xtrain shape: ', Xtrain.shape) \n","print('after transpose: Xtest  shape: ', Xtest.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OE2GPzLIaMgG","executionInfo":{"status":"ok","timestamp":1673595118953,"user_tz":-480,"elapsed":4959,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"97faa434-1c28-4d75-e457-ea5abfe61f2d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperspectral data shape:  (145, 145, 200)\n","Label shape:  (145, 145)\n","\n","... ... PCA tranformation ... ...\n","Data shape after PCA:  (145, 145, 30)\n","\n","... ... create data cubes ... ...\n","Data cube X shape:  (10249, 21, 21, 30)\n","Data cube y shape:  (10249,)\n","\n","... ... create train & test data ... ...\n","Xtrain shape:  (1024, 21, 21, 30)\n","Xtest  shape:  (9225, 21, 21, 30)\n","after transpose: Xtrain shape:  (1024, 30, 21, 21)\n","after transpose: Xtest  shape:  (9225, 30, 21, 21)\n"]}]},{"cell_type":"code","source":["\"\"\" Training dataset\"\"\"\n","class TrainDS(torch.utils.data.Dataset): \n","    def __init__(self):\n","        self.len = Xtrain.shape[0]\n","        self.x_data = torch.FloatTensor(Xtrain)\n","        self.y_data = torch.LongTensor(ytrain)        \n","    def __getitem__(self, index):\n","        # 根据索引返回数据和对应的标签\n","        return self.x_data[index], self.y_data[index]\n","    def __len__(self): \n","        # 返回文件数据的数目\n","        return self.len\n","\n","\"\"\" Testing dataset\"\"\"\n","class TestDS(torch.utils.data.Dataset): \n","    def __init__(self):\n","        self.len = Xtest.shape[0]\n","        self.x_data = torch.FloatTensor(Xtest)\n","        self.y_data = torch.LongTensor(ytest)\n","    def __getitem__(self, index):\n","        # 根据索引返回数据和对应的标签\n","        return self.x_data[index], self.y_data[index]\n","    def __len__(self): \n","        # 返回文件数据的数目\n","        return self.len\n","\n","\"\"\" Validing dataset\"\"\"\n","class ValidDS(torch.utils.data.Dataset): \n","    def __init__(self):\n","        self.len = Xvalid.shape[0]\n","        self.x_data = torch.FloatTensor(Xvalid)\n","        self.y_data = torch.LongTensor(yvalid)\n","    def __getitem__(self, index):\n","        # 根据索引返回数据和对应的标签\n","        return self.x_data[index], self.y_data[index]\n","    def __len__(self): \n","        # 返回文件数据的数目\n","        return self.len\n","\n","\n","# 创建 trainloader 和 testloader\n","trainset = TrainDS()\n","testset  = TestDS()\n","\n","train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=64, shuffle=True, num_workers=2)\n","test_loader  = torch.utils.data.DataLoader(dataset=testset,  batch_size=64, shuffle=False, num_workers=2)\n"],"metadata":{"id":"aDzJMSgGaQcX","executionInfo":{"status":"ok","timestamp":1673595118954,"user_tz":-480,"elapsed":7,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","from torch import nn, optim\n","from torch.autograd import Variable\n","def train(net):\n","\n","\n","  current_loss_his = []\n","  current_Acc_his = []\n","\n","  best_net_wts = copy.deepcopy(net.state_dict())\n","  best_acc = 0.0\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(net.parameters(), lr=0.0003)\n","  step_schedule = optim.lr_scheduler.StepLR(step_size=20, gamma=0.8, optimizer=optimizer)\n","\n","\n","  # 开始训练\n","  total_loss = 0\n","  for epoch in range(100):\n","      net.train()  # 将模型设置为训练模式\n","      for i, (inputs, labels) in enumerate(train_loader):\n","          inputs = inputs.to(device)\n","          labels = labels.to(device)\n","          # 优化器梯度归零\n","          optimizer.zero_grad()\n","          # 正向传播 +　反向传播 + 优化 \n","          outputs = net(inputs)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","          total_loss += loss.item()\n","      step_schedule.step()\n","\n","      net.eval()   # 将模型设置为验证模式\n","      current_acc = test_acc(net)\n","      current_Acc_his.append(current_acc)\n","\n","      if current_acc > best_acc:\n","        best_acc = current_acc\n","        best_net_wts = copy.deepcopy(net.state_dict())\n","      \n","      print('[Epoch: %d]   [learning rate: %.6f]   [loss avg: %.4f]   [current loss: %.4f]  [current acc: %.4f]' %(epoch + 1, step_schedule.get_last_lr()[0], total_loss/(epoch+1), loss.item(), current_acc))\n","      current_loss_his.append(loss.item())\n","\n","  print('Finished Training')\n","  print(\"Best Acc:%.4f\" %(best_acc))\n","\n","  # load best model weights\n","  net.load_state_dict(best_net_wts)\n","\n","  return net,current_loss_his,current_Acc_his"],"metadata":{"id":"BRfGAWjfaT4l","executionInfo":{"status":"ok","timestamp":1673595118954,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def test_acc(net):\n","  count = 0\n","  # 模型测试\n","  for inputs, _ in test_loader:\n","      inputs = inputs.to(device)\n","      outputs = net(inputs)\n","      outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n","      if count == 0:\n","          y_pred_test =  outputs\n","          count = 1\n","      else:\n","          y_pred_test = np.concatenate( (y_pred_test, outputs) )\n","\n","  # 生成分类报告\n","  classification = classification_report(ytest, y_pred_test, digits=4)\n","  index_acc = classification.find('weighted avg')\n","  accuracy = classification[index_acc+17:index_acc+23]\n","  return float(accuracy)"],"metadata":{"id":"eI1EA02VaVwF","executionInfo":{"status":"ok","timestamp":1673595118954,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# 使用GPU训练，可以在菜单 \"代码执行工具\" -> \"更改运行时类型\" 里进行设置\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 网络放到GPU上\n","net = SwinTransformer().to(device)\n","# 训练\n","net,current_loss_his,current_Acc_his = train(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaCyq8gbaXWG","executionInfo":{"status":"ok","timestamp":1673596426959,"user_tz":-480,"elapsed":1308009,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"ed1ee8b6-80a7-43a1-c593-199ee9d2d89f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch: 1]   [learning rate: 0.000300]   [loss avg: 26.9154]   [current loss: 1.0033]  [current acc: 0.7210]\n","[Epoch: 2]   [learning rate: 0.000300]   [loss avg: 19.3613]   [current loss: 0.7764]  [current acc: 0.8158]\n","[Epoch: 3]   [learning rate: 0.000300]   [loss avg: 15.5170]   [current loss: 0.4072]  [current acc: 0.8401]\n","[Epoch: 4]   [learning rate: 0.000300]   [loss avg: 12.9516]   [current loss: 0.2993]  [current acc: 0.8688]\n","[Epoch: 5]   [learning rate: 0.000300]   [loss avg: 11.0326]   [current loss: 0.1303]  [current acc: 0.9231]\n","[Epoch: 6]   [learning rate: 0.000300]   [loss avg: 9.4591]   [current loss: 0.0536]  [current acc: 0.9430]\n","[Epoch: 7]   [learning rate: 0.000300]   [loss avg: 8.2708]   [current loss: 0.0831]  [current acc: 0.8999]\n","[Epoch: 8]   [learning rate: 0.000300]   [loss avg: 7.3884]   [current loss: 0.0292]  [current acc: 0.9319]\n","[Epoch: 9]   [learning rate: 0.000300]   [loss avg: 6.6368]   [current loss: 0.0697]  [current acc: 0.9348]\n","[Epoch: 10]   [learning rate: 0.000300]   [loss avg: 6.0159]   [current loss: 0.0120]  [current acc: 0.9344]\n","[Epoch: 11]   [learning rate: 0.000300]   [loss avg: 5.4983]   [current loss: 0.0121]  [current acc: 0.9534]\n","[Epoch: 12]   [learning rate: 0.000300]   [loss avg: 5.0571]   [current loss: 0.0021]  [current acc: 0.9507]\n","[Epoch: 13]   [learning rate: 0.000300]   [loss avg: 4.6757]   [current loss: 0.0043]  [current acc: 0.9556]\n","[Epoch: 14]   [learning rate: 0.000300]   [loss avg: 4.3456]   [current loss: 0.0017]  [current acc: 0.9585]\n","[Epoch: 15]   [learning rate: 0.000300]   [loss avg: 4.0585]   [current loss: 0.0011]  [current acc: 0.9610]\n","[Epoch: 16]   [learning rate: 0.000300]   [loss avg: 3.8066]   [current loss: 0.0012]  [current acc: 0.9609]\n","[Epoch: 17]   [learning rate: 0.000300]   [loss avg: 3.5842]   [current loss: 0.0010]  [current acc: 0.9636]\n","[Epoch: 18]   [learning rate: 0.000300]   [loss avg: 3.3863]   [current loss: 0.0010]  [current acc: 0.9646]\n","[Epoch: 19]   [learning rate: 0.000300]   [loss avg: 3.2092]   [current loss: 0.0014]  [current acc: 0.9663]\n","[Epoch: 20]   [learning rate: 0.000240]   [loss avg: 3.0499]   [current loss: 0.0005]  [current acc: 0.9668]\n","[Epoch: 21]   [learning rate: 0.000240]   [loss avg: 2.9056]   [current loss: 0.0008]  [current acc: 0.9652]\n","[Epoch: 22]   [learning rate: 0.000240]   [loss avg: 2.7742]   [current loss: 0.0011]  [current acc: 0.9643]\n","[Epoch: 23]   [learning rate: 0.000240]   [loss avg: 2.6544]   [current loss: 0.0005]  [current acc: 0.9637]\n","[Epoch: 24]   [learning rate: 0.000240]   [loss avg: 2.5445]   [current loss: 0.0011]  [current acc: 0.9644]\n","[Epoch: 25]   [learning rate: 0.000240]   [loss avg: 2.4434]   [current loss: 0.0005]  [current acc: 0.9641]\n","[Epoch: 26]   [learning rate: 0.000240]   [loss avg: 2.3499]   [current loss: 0.0003]  [current acc: 0.9640]\n","[Epoch: 27]   [learning rate: 0.000240]   [loss avg: 2.2633]   [current loss: 0.0016]  [current acc: 0.9650]\n","[Epoch: 28]   [learning rate: 0.000240]   [loss avg: 2.1828]   [current loss: 0.0004]  [current acc: 0.9649]\n","[Epoch: 29]   [learning rate: 0.000240]   [loss avg: 2.1079]   [current loss: 0.0006]  [current acc: 0.9648]\n","[Epoch: 30]   [learning rate: 0.000240]   [loss avg: 2.0380]   [current loss: 0.0004]  [current acc: 0.9639]\n","[Epoch: 31]   [learning rate: 0.000240]   [loss avg: 1.9726]   [current loss: 0.0004]  [current acc: 0.9638]\n","[Epoch: 32]   [learning rate: 0.000240]   [loss avg: 1.9113]   [current loss: 0.0013]  [current acc: 0.9649]\n","[Epoch: 33]   [learning rate: 0.000240]   [loss avg: 1.8537]   [current loss: 0.0009]  [current acc: 0.9642]\n","[Epoch: 34]   [learning rate: 0.000240]   [loss avg: 1.7995]   [current loss: 0.0004]  [current acc: 0.9645]\n","[Epoch: 35]   [learning rate: 0.000240]   [loss avg: 1.7485]   [current loss: 0.0005]  [current acc: 0.9654]\n","[Epoch: 36]   [learning rate: 0.000240]   [loss avg: 1.7001]   [current loss: 0.0022]  [current acc: 0.9646]\n","[Epoch: 37]   [learning rate: 0.000240]   [loss avg: 1.6544]   [current loss: 0.0003]  [current acc: 0.9664]\n","[Epoch: 38]   [learning rate: 0.000240]   [loss avg: 1.6110]   [current loss: 0.0003]  [current acc: 0.9653]\n","[Epoch: 39]   [learning rate: 0.000240]   [loss avg: 1.5699]   [current loss: 0.0005]  [current acc: 0.9651]\n","[Epoch: 40]   [learning rate: 0.000192]   [loss avg: 1.5309]   [current loss: 0.0003]  [current acc: 0.9648]\n","[Epoch: 41]   [learning rate: 0.000192]   [loss avg: 1.4938]   [current loss: 0.0004]  [current acc: 0.9649]\n","[Epoch: 42]   [learning rate: 0.000192]   [loss avg: 1.4584]   [current loss: 0.0003]  [current acc: 0.9642]\n","[Epoch: 43]   [learning rate: 0.000192]   [loss avg: 1.4246]   [current loss: 0.0003]  [current acc: 0.9646]\n","[Epoch: 44]   [learning rate: 0.000192]   [loss avg: 1.3924]   [current loss: 0.0003]  [current acc: 0.9645]\n","[Epoch: 45]   [learning rate: 0.000192]   [loss avg: 1.3615]   [current loss: 0.0002]  [current acc: 0.9644]\n","[Epoch: 46]   [learning rate: 0.000192]   [loss avg: 1.3321]   [current loss: 0.0004]  [current acc: 0.9640]\n","[Epoch: 47]   [learning rate: 0.000192]   [loss avg: 1.3039]   [current loss: 0.0002]  [current acc: 0.9643]\n","[Epoch: 48]   [learning rate: 0.000192]   [loss avg: 1.2768]   [current loss: 0.0004]  [current acc: 0.9645]\n","[Epoch: 49]   [learning rate: 0.000192]   [loss avg: 1.2509]   [current loss: 0.0007]  [current acc: 0.9637]\n","[Epoch: 50]   [learning rate: 0.000192]   [loss avg: 1.2263]   [current loss: 0.0012]  [current acc: 0.9646]\n","[Epoch: 51]   [learning rate: 0.000192]   [loss avg: 1.2024]   [current loss: 0.0010]  [current acc: 0.9626]\n","[Epoch: 52]   [learning rate: 0.000192]   [loss avg: 1.1796]   [current loss: 0.0003]  [current acc: 0.9563]\n","[Epoch: 53]   [learning rate: 0.000192]   [loss avg: 1.1948]   [current loss: 0.2464]  [current acc: 0.8168]\n","[Epoch: 54]   [learning rate: 0.000192]   [loss avg: 1.3508]   [current loss: 0.6678]  [current acc: 0.8441]\n","[Epoch: 55]   [learning rate: 0.000192]   [loss avg: 1.4016]   [current loss: 0.2118]  [current acc: 0.9220]\n","[Epoch: 56]   [learning rate: 0.000192]   [loss avg: 1.3971]   [current loss: 0.0511]  [current acc: 0.9423]\n","[Epoch: 57]   [learning rate: 0.000192]   [loss avg: 1.3791]   [current loss: 0.0268]  [current acc: 0.9541]\n","[Epoch: 58]   [learning rate: 0.000192]   [loss avg: 1.3592]   [current loss: 0.0168]  [current acc: 0.9498]\n","[Epoch: 59]   [learning rate: 0.000192]   [loss avg: 1.3384]   [current loss: 0.0039]  [current acc: 0.9600]\n","[Epoch: 60]   [learning rate: 0.000154]   [loss avg: 1.3173]   [current loss: 0.0020]  [current acc: 0.9609]\n","[Epoch: 61]   [learning rate: 0.000154]   [loss avg: 1.2963]   [current loss: 0.0014]  [current acc: 0.9628]\n","[Epoch: 62]   [learning rate: 0.000154]   [loss avg: 1.2759]   [current loss: 0.0009]  [current acc: 0.9642]\n","[Epoch: 63]   [learning rate: 0.000154]   [loss avg: 1.2562]   [current loss: 0.0086]  [current acc: 0.9653]\n","[Epoch: 64]   [learning rate: 0.000154]   [loss avg: 1.2371]   [current loss: 0.0018]  [current acc: 0.9651]\n","[Epoch: 65]   [learning rate: 0.000154]   [loss avg: 1.2184]   [current loss: 0.0029]  [current acc: 0.9661]\n","[Epoch: 66]   [learning rate: 0.000154]   [loss avg: 1.2003]   [current loss: 0.0015]  [current acc: 0.9664]\n","[Epoch: 67]   [learning rate: 0.000154]   [loss avg: 1.1827]   [current loss: 0.0010]  [current acc: 0.9657]\n","[Epoch: 68]   [learning rate: 0.000154]   [loss avg: 1.1656]   [current loss: 0.0014]  [current acc: 0.9664]\n","[Epoch: 69]   [learning rate: 0.000154]   [loss avg: 1.1493]   [current loss: 0.0016]  [current acc: 0.9652]\n","[Epoch: 70]   [learning rate: 0.000154]   [loss avg: 1.1332]   [current loss: 0.0011]  [current acc: 0.9638]\n","[Epoch: 71]   [learning rate: 0.000154]   [loss avg: 1.1175]   [current loss: 0.0013]  [current acc: 0.9645]\n","[Epoch: 72]   [learning rate: 0.000154]   [loss avg: 1.1023]   [current loss: 0.0007]  [current acc: 0.9647]\n","[Epoch: 73]   [learning rate: 0.000154]   [loss avg: 1.0874]   [current loss: 0.0006]  [current acc: 0.9650]\n","[Epoch: 74]   [learning rate: 0.000154]   [loss avg: 1.0730]   [current loss: 0.0010]  [current acc: 0.9643]\n","[Epoch: 75]   [learning rate: 0.000154]   [loss avg: 1.0589]   [current loss: 0.0011]  [current acc: 0.9647]\n","[Epoch: 76]   [learning rate: 0.000154]   [loss avg: 1.0452]   [current loss: 0.0017]  [current acc: 0.9660]\n","[Epoch: 77]   [learning rate: 0.000154]   [loss avg: 1.0318]   [current loss: 0.0010]  [current acc: 0.9668]\n","[Epoch: 78]   [learning rate: 0.000154]   [loss avg: 1.0188]   [current loss: 0.0007]  [current acc: 0.9674]\n","[Epoch: 79]   [learning rate: 0.000154]   [loss avg: 1.0060]   [current loss: 0.0006]  [current acc: 0.9673]\n","[Epoch: 80]   [learning rate: 0.000123]   [loss avg: 0.9936]   [current loss: 0.0006]  [current acc: 0.9663]\n","[Epoch: 81]   [learning rate: 0.000123]   [loss avg: 0.9815]   [current loss: 0.0005]  [current acc: 0.9647]\n","[Epoch: 82]   [learning rate: 0.000123]   [loss avg: 0.9699]   [current loss: 0.0008]  [current acc: 0.9650]\n","[Epoch: 83]   [learning rate: 0.000123]   [loss avg: 0.9583]   [current loss: 0.0006]  [current acc: 0.9650]\n","[Epoch: 84]   [learning rate: 0.000123]   [loss avg: 0.9471]   [current loss: 0.0008]  [current acc: 0.9651]\n","[Epoch: 85]   [learning rate: 0.000123]   [loss avg: 0.9361]   [current loss: 0.0014]  [current acc: 0.9642]\n","[Epoch: 86]   [learning rate: 0.000123]   [loss avg: 0.9254]   [current loss: 0.0003]  [current acc: 0.9638]\n","[Epoch: 87]   [learning rate: 0.000123]   [loss avg: 0.9149]   [current loss: 0.0012]  [current acc: 0.9651]\n","[Epoch: 88]   [learning rate: 0.000123]   [loss avg: 0.9046]   [current loss: 0.0006]  [current acc: 0.9648]\n","[Epoch: 89]   [learning rate: 0.000123]   [loss avg: 0.8946]   [current loss: 0.0004]  [current acc: 0.9650]\n","[Epoch: 90]   [learning rate: 0.000123]   [loss avg: 0.8848]   [current loss: 0.0012]  [current acc: 0.9652]\n","[Epoch: 91]   [learning rate: 0.000123]   [loss avg: 0.8752]   [current loss: 0.0004]  [current acc: 0.9657]\n","[Epoch: 92]   [learning rate: 0.000123]   [loss avg: 0.8658]   [current loss: 0.0018]  [current acc: 0.9639]\n","[Epoch: 93]   [learning rate: 0.000123]   [loss avg: 0.8566]   [current loss: 0.0007]  [current acc: 0.9648]\n","[Epoch: 94]   [learning rate: 0.000123]   [loss avg: 0.8476]   [current loss: 0.0008]  [current acc: 0.9655]\n","[Epoch: 95]   [learning rate: 0.000123]   [loss avg: 0.8388]   [current loss: 0.0007]  [current acc: 0.9666]\n","[Epoch: 96]   [learning rate: 0.000123]   [loss avg: 0.8302]   [current loss: 0.0006]  [current acc: 0.9640]\n","[Epoch: 97]   [learning rate: 0.000123]   [loss avg: 0.8218]   [current loss: 0.0011]  [current acc: 0.9655]\n","[Epoch: 98]   [learning rate: 0.000123]   [loss avg: 0.8135]   [current loss: 0.0002]  [current acc: 0.9659]\n","[Epoch: 99]   [learning rate: 0.000123]   [loss avg: 0.8053]   [current loss: 0.0005]  [current acc: 0.9668]\n","[Epoch: 100]   [learning rate: 0.000098]   [loss avg: 0.7974]   [current loss: 0.0005]  [current acc: 0.9666]\n","Finished Training\n","Best Acc:0.9674\n"]}]},{"cell_type":"code","source":["net.eval()   # 将模型设置为验证模式\n","# 测试最好的模型的结果\n","count = 0\n","# 模型测试\n","for inputs, _ in test_loader:\n","    inputs = inputs.to(device)\n","    outputs = net(inputs)\n","    outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n","    if count == 0:\n","        y_pred_test =  outputs\n","        count = 1\n","    else:\n","        y_pred_test = np.concatenate( (y_pred_test, outputs) )\n","\n","# 生成分类报告\n","classification = classification_report(ytest, y_pred_test, digits=4)\n","print(classification)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzG5W6T038WO","executionInfo":{"status":"ok","timestamp":1673596436659,"user_tz":-480,"elapsed":9707,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"53291a32-4f43-40ae-fe46-d7e704f75c72"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         0.0     0.9268    0.9268    0.9268        41\n","         1.0     0.9507    0.9152    0.9326      1285\n","         2.0     0.9408    0.9786    0.9593       747\n","         3.0     0.9442    0.9531    0.9486       213\n","         4.0     0.9907    0.9793    0.9850       435\n","         5.0     0.9805    0.9954    0.9879       657\n","         6.0     1.0000    1.0000    1.0000        25\n","         7.0     1.0000    1.0000    1.0000       430\n","         8.0     0.9474    1.0000    0.9730        18\n","         9.0     0.9710    0.9577    0.9643       875\n","        10.0     0.9659    0.9882    0.9770      2210\n","        11.0     0.9264    0.9195    0.9229       534\n","        12.0     0.9946    0.9946    0.9946       185\n","        13.0     0.9974    0.9991    0.9982      1139\n","        14.0     0.9909    0.9452    0.9676       347\n","        15.0     0.8356    0.7262    0.7771        84\n","\n","    accuracy                         0.9675      9225\n","   macro avg     0.9602    0.9549    0.9572      9225\n","weighted avg     0.9674    0.9675    0.9672      9225\n","\n"]}]},{"cell_type":"code","source":["# load the original image\n","X = sio.loadmat('/content/drive/MyDrive/AI data/hyperspetral image/Indian_pines_corrected.mat')['indian_pines_corrected']\n","y = sio.loadmat('/content/drive/MyDrive/AI data/hyperspetral image/Indian_pines_gt.mat')['indian_pines_gt']\n","\n","height = y.shape[0]\n","width = y.shape[1]\n","print(\"height:\",height)\n","print(\"width:\",width)\n","\n","X = applyPCA(X, numComponents= pca_components)\n","X = padWithZeros(X, 21//2)\n","\n","# 逐像素预测类别\n","outputs = np.zeros((height,width))\n","for i in range(height):\n","    for j in range(width):\n","        if int(y[i,j]) == 0:\n","            continue\n","        else :\n","            image_patch = X[i:i+patch_size, j:j+patch_size, :]\n","            #print(image_patch.shape)\n","            image_patch = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2])\n","            #print(image_patch.shape)\n","            X_test_image = torch.FloatTensor(image_patch.transpose(0, 3, 1, 2)).to(device)                                   \n","            prediction = net(X_test_image)\n","            prediction = np.argmax(prediction.detach().cpu().numpy(), axis=1)\n","            outputs[i][j] = prediction+1\n","    if i % 20 == 0:\n","        print('... ... row ', i, ' handling ... ...')"],"metadata":{"id":"KqEf7g434Dg2","executionInfo":{"status":"ok","timestamp":1673596616102,"user_tz":-480,"elapsed":179449,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9b22ead-d9e2-4406-90e3-1ac15734265b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["height: 145\n","width: 145\n","... ... row  0  handling ... ...\n","... ... row  20  handling ... ...\n","... ... row  40  handling ... ...\n","... ... row  60  handling ... ...\n","... ... row  80  handling ... ...\n","... ... row  100  handling ... ...\n","... ... row  120  handling ... ...\n","... ... row  140  handling ... ...\n"]}]},{"cell_type":"code","source":["hight=145\n","width=145\n","num_correct = 0\n","num_false = 0\n","for i in range(hight):\n","  for j in range(width):\n","    if outputs[i,j] == y[i,j]:\n","      num_correct += 1\n","    if outputs[i,j] != y[i,j]:\n","      num_false += 1\n","print(\"预测正确的像素个数为：{} | 预测错误的个数为：{} | 预测准确率：{}%\".format(int(num_correct),int(num_false),float((num_correct/(145*145)*100))))"],"metadata":{"id":"AFRgTOWC7Bfr","executionInfo":{"status":"ok","timestamp":1673596616103,"user_tz":-480,"elapsed":29,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a8ae2fb-fdc4-45c5-bbb9-5dd76dccd19c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["预测正确的像素个数为：20710 | 预测错误的个数为：315 | 预测准确率：98.50178359096314%\n"]}]},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","import numpy as np\n","from typing import Optional\n","\n","\n","def drop_path_f(x, drop_prob: float = 0., training: bool = False):\n","    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n","\n","    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n","    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n","    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n","    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n","    'survival rate' as the argument.\n","\n","    \"\"\"\n","    if drop_prob == 0. or not training:\n","        return x\n","    keep_prob = 1 - drop_prob\n","    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n","    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n","    random_tensor.floor_()  # binarize\n","    output = x.div(keep_prob) * random_tensor\n","    return output\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","    def __init__(self, drop_prob=None):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path_f(x, self.drop_prob, self.training)\n","\n","\n","def window_partition(x, window_size: int):\n","    \"\"\"\n","    将feature map按照window_size划分成一个个没有重叠的window\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size(M)\n","\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    # permute: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H//Mh, W//Mh, Mw, Mw, C]\n","    # view: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B*num_windows, Mh, Mw, C]\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size: int, H: int, W: int):\n","    \"\"\"\n","    将一个个window还原成一个feature map\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size(M)\n","        H (int): Height of image\n","        W (int): Width of image\n","\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    # view: [B*num_windows, Mh, Mw, C] -> [B, H//Mh, W//Mw, Mh, Mw, C]\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    # permute: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B, H//Mh, Mh, W//Mw, Mw, C]\n","    # view: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H, W, C]\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\"\n","    2D Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, patch_size=4, in_c=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        patch_size = (patch_size, patch_size)\n","        self.patch_size = patch_size\n","        self.in_chans = in_c\n","        self.embed_dim = embed_dim\n","        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n","\n","    def forward(self, x):\n","        _, _, H, W = x.shape\n","\n","        # padding\n","        # 如果输入图片的H，W不是patch_size的整数倍，需要进行padding\n","        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)\n","        if pad_input:\n","            # to pad the last 3 dimensions,\n","            # (W_left, W_right, H_top,H_bottom, C_front, C_back)\n","            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],\n","                          0, self.patch_size[0] - H % self.patch_size[0],\n","                          0, 0))\n","\n","        # 下采样patch_size倍\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        # flatten: [B, C, H, W] -> [B, C, HW]\n","        # transpose: [B, C, HW] -> [B, HW, C]\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","        return x, H, W\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x, H, W):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        # padding\n","        # 如果输入feature map的H，W不是2的整数倍，需要进行padding\n","        pad_input = (H % 2 == 1) or (W % 2 == 1)\n","        if pad_input:\n","            # to pad the last 3 dimensions, starting from the last dimension and moving forward.\n","            # (C_front, C_back, W_left, W_right, H_top, H_bottom)\n","            # 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同\n","            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n","\n","        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]\n","        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]\n","        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]\n","        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]\n","        x = torch.cat([x0, x1, x2, x3], -1)  # [B, H/2, W/2, 4*C]\n","        x = x.view(B, -1, 4 * C)  # [B, H/2*W/2, 4*C]\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)  # [B, H/2*W/2, 2*C]\n","\n","        return x\n","\n","\n","class Mlp(nn.Module):\n","    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n","    \"\"\"\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.drop1 = nn.Dropout(drop)\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop2 = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop1(x)\n","        x = self.fc2(x)\n","        x = self.drop2(x)\n","        return x\n","\n","\n","class WindowAttention(nn.Module):\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # [Mh, Mw]\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Mh-1 * 2*Mw-1, nH]\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # [2, Mh, Mw]\n","        coords_flatten = torch.flatten(coords, 1)  # [2, Mh*Mw]\n","        # [2, Mh*Mw, 1] - [2, 1, Mh*Mw]\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # [2, Mh*Mw, Mh*Mw]\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # [Mh*Mw, Mh*Mw, 2]\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # [Mh*Mw, Mh*Mw]\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n","\n","    def forward(self, x, mask: Optional[torch.Tensor] = None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, Mh*Mw, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        # [batch_size*num_windows, Mh*Mw, total_embed_dim]\n","        B_, N, C = x.shape\n","        # qkv(): -> [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]\n","        # reshape: -> [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]\n","        # permute: -> [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        # [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #下面的q,k,v的shape如上注释所示，其意义是：\n","        #batch_size*num_windows表示所有数据的总窗口数，就相当于ViT中的batch_size\n","        #num_heads在这里可以不用管\n","        #Mh*Mw表示窗口的长乘宽。意义相当于patch的个数或者长度\n","        #embed_dim_per_head表示dim经过分头后的长度。意义相当于patch的个数或者长度\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        #综上所述该网络是将所有图片的所有window看作一个数据整体（相当于ViT中的batch_size），Mh*Mw和embed_dim_per_head看作是每个window的多个具有长度的patch。从而计算每张图片中每个window中的注意力（通俗理解：网络计算的是每个window中内部点位之间的相关性）\n","        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n","\n","        #****************************ChannelAttention**************************\n","        q_c, k_c, v_c = qkv[0], qkv[1], qkv[2]\n","\n","        q_c = q_c.transpose(-2, -1)\n","        k_c = k_c.transpose(-2, -1)\n","        v_c = v_c.transpose(-2, -1)\n","\n","        q_c = F.normalize(q_c, dim=-1)\n","        k_c = F.normalize(k_c, dim=-1)\n","\n","        attn_c = (q_c @ k_c.transpose(-2, -1)) * self.temperature\n","        attn_c = attn_c.softmax(dim=-1)\n","        attn_c = self.attn_drop(attn_c)\n","        x_c = (attn_c @ v_c).permute(0, 3, 1, 2).reshape(B_, N, C)\n","        x_c = self.proj(x_c)\n","        x_c = self.proj_drop(x_c)\n","        #****************************ChannelAttention had down**************************\n","\n","        # transpose: -> [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]\n","        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        # relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -> [Mh*Mw,Mh*Mw,nH]\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # [nH, Mh*Mw, Mh*Mw]\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            # mask: [nW, Mh*Mw, Mh*Mw]\n","            nW = mask.shape[0]  # num_windows\n","            # attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]\n","            # mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n","        # transpose: -> [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]\n","        # reshape: -> [batch_size*num_windows, Mh*Mw, total_embed_dim]\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        #x = x + x_c\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        x = x + x_c\n","        \n","        return x\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    r\"\"\" Swin Transformer Block.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n","                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,\n","            attn_drop=attn_drop, proj_drop=drop)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","    def forward(self, x, attn_mask):\n","        H, W = self.H, self.W\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # pad feature maps to multiples of window size\n","        # 把feature map给pad到window size的整数倍\n","        pad_l = pad_t = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = x.shape\n","\n","        # cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","\n","        # partition windows\n","        x_windows = window_partition(shifted_x, self.window_size)  # [nW*B, Mh, Mw, C]\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # [nW*B, Mh*Mw, C]\n","\n","        # W-MSA/SW-MSA\n","        attn_windows = self.attn(x_windows, mask=attn_mask)  # [nW*B, Mh*Mw, C]\n","\n","        # merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)  # [nW*B, Mh, Mw, C]\n","        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # [B, H', W', C]\n","\n","        # reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if pad_r > 0 or pad_b > 0:\n","            # 把前面pad的数据移除掉\n","            x = x[:, :H, :W, :].contiguous()\n","\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\"\n","    A basic Swin Transformer layer for one stage.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","    \"\"\"\n","\n","    def __init__(self, dim, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n","        super().__init__()\n","        self.dim = dim\n","        self.depth = depth\n","        self.window_size = window_size\n","        self.use_checkpoint = use_checkpoint\n","        self.shift_size = window_size // 2\n","\n","        # build blocks\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(\n","                dim=dim,\n","                num_heads=num_heads,\n","                window_size=window_size,\n","                shift_size=0 if (i % 2 == 0) else self.shift_size,\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                drop=drop,\n","                attn_drop=attn_drop,\n","                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                norm_layer=norm_layer)\n","            for i in range(depth)])\n","\n","        # patch merging layer\n","        if downsample is not None:\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def create_mask(self, x, H, W):\n","        # calculate attention mask for SW-MSA\n","        # 保证Hp和Wp是window_size的整数倍\n","        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n","        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n","        # 拥有和feature map一样的通道排列顺序，方便后续window_partition\n","        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # [1, Hp, Wp, 1]\n","        h_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        w_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)  # [nW, Mh, Mw, 1]\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)  # [nW, Mh*Mw]\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]\n","        # [nW, Mh*Mw, Mh*Mw]\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","        return attn_mask\n","\n","    def forward(self, x, H, W):\n","        attn_mask = self.create_mask(x, H, W)  # [nW, Mh*Mw, Mh*Mw]\n","        for blk in self.blocks:\n","            blk.H, blk.W = H, W\n","            if not torch.jit.is_scripting() and self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x, attn_mask)\n","            else:\n","                x = blk(x, attn_mask)\n","        if self.downsample is not None:\n","            x = self.downsample(x, H, W)\n","            H, W = (H + 1) // 2, (W + 1) // 2\n","\n","        return x, H, W\n","\n","\n","class spe_SwinTransformer(nn.Module):\n","    r\"\"\" Swin Transformer\n","        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n","          https://arxiv.org/pdf/2103.14030\n","\n","    Args:\n","        patch_size (int | tuple(int)): Patch size. Default: 4\n","        in_chans (int): Number of input image channels. Default: 3\n","        num_classes (int): Number of classes for classification head. Default: 1000\n","        embed_dim (int): Patch embedding dimension. Default: 96\n","        depths (tuple(int)): Depth of each Swin Transformer layer.\n","        num_heads (tuple(int)): Number of attention heads in different layers.\n","        window_size (int): Window size. Default: 7\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n","        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n","        drop_rate (float): Dropout rate. Default: 0\n","        attn_drop_rate (float): Attention dropout rate. Default: 0\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n","        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n","        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n","    \"\"\"\n","\n","    def __init__(self, patch_size=1, in_chans=30, num_classes=16,\n","                 embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24),\n","                 window_size=7, mlp_ratio=4., qkv_bias=True,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n","                 norm_layer=nn.LayerNorm, patch_norm=True,\n","                 use_checkpoint=False, **kwargs):\n","        super().__init__()\n","\n","        self.num_classes = num_classes\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.patch_norm = patch_norm\n","        # stage4输出特征矩阵的channels\n","        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","        self.mlp_ratio = mlp_ratio\n","\n","        # split image into non-overlapping patches\n","        self.patch_embed = PatchEmbed(\n","            patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","\n","        # build layers\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            # 注意这里构建的stage和论文图中有些差异\n","            # 这里的stage不包含该stage的patch_merging层，包含的是下个stage的\n","            layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n","                                depth=depths[i_layer],\n","                                num_heads=num_heads[i_layer],\n","                                window_size=window_size,\n","                                mlp_ratio=self.mlp_ratio,\n","                                qkv_bias=qkv_bias,\n","                                drop=drop_rate,\n","                                attn_drop=attn_drop_rate,\n","                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n","                                norm_layer=norm_layer,\n","                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n","                                use_checkpoint=use_checkpoint)\n","            self.layers.append(layers)\n","\n","        self.norm = norm_layer(self.num_features)\n","        self.avgpool = nn.AdaptiveAvgPool1d(1)\n","        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, x):\n","        # x: [B, L, C]\n","        x, H, W = self.patch_embed(x)\n","        x = self.pos_drop(x)\n","\n","        for layer in self.layers:\n","            x, H, W = layer(x, H, W)\n","\n","        x = self.norm(x)  # [B, L, C]\n","        x = self.avgpool(x.transpose(1, 2))  # [B, C, 1]\n","        x = torch.flatten(x, 1)\n","        x = self.head(x)\n","        return x\n","\n","\n"],"metadata":{"id":"WhvHL9KWAbVM","executionInfo":{"status":"ok","timestamp":1673596616104,"user_tz":-480,"elapsed":27,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# 使用GPU训练，可以在菜单 \"代码执行工具\" -> \"更改运行时类型\" 里进行设置\n","#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 网络放到GPU上\n","net1 = spe_SwinTransformer().to(device)\n","# 训练\n","net1,current_loss_his,current_Acc_his = train(net1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNcCONoQBB4N","executionInfo":{"status":"ok","timestamp":1673598318554,"user_tz":-480,"elapsed":1702476,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"fb7adf90-64ee-4f5e-b96f-f53034edb313"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch: 1]   [learning rate: 0.000300]   [loss avg: 26.1852]   [current loss: 0.9361]  [current acc: 0.7351]\n","[Epoch: 2]   [learning rate: 0.000300]   [loss avg: 18.4974]   [current loss: 0.6671]  [current acc: 0.8138]\n","[Epoch: 3]   [learning rate: 0.000300]   [loss avg: 14.5816]   [current loss: 0.2957]  [current acc: 0.8873]\n","[Epoch: 4]   [learning rate: 0.000300]   [loss avg: 12.0075]   [current loss: 0.3975]  [current acc: 0.8888]\n","[Epoch: 5]   [learning rate: 0.000300]   [loss avg: 10.2594]   [current loss: 0.2029]  [current acc: 0.8978]\n","[Epoch: 6]   [learning rate: 0.000300]   [loss avg: 8.9287]   [current loss: 0.0582]  [current acc: 0.9224]\n","[Epoch: 7]   [learning rate: 0.000300]   [loss avg: 8.0259]   [current loss: 0.2860]  [current acc: 0.9071]\n","[Epoch: 8]   [learning rate: 0.000300]   [loss avg: 7.2636]   [current loss: 0.0792]  [current acc: 0.9341]\n","[Epoch: 9]   [learning rate: 0.000300]   [loss avg: 6.5639]   [current loss: 0.0917]  [current acc: 0.9395]\n","[Epoch: 10]   [learning rate: 0.000300]   [loss avg: 6.0852]   [current loss: 0.0721]  [current acc: 0.9156]\n","[Epoch: 11]   [learning rate: 0.000300]   [loss avg: 5.6225]   [current loss: 0.0260]  [current acc: 0.9310]\n","[Epoch: 12]   [learning rate: 0.000300]   [loss avg: 5.1955]   [current loss: 0.0224]  [current acc: 0.9536]\n","[Epoch: 13]   [learning rate: 0.000300]   [loss avg: 4.8302]   [current loss: 0.0739]  [current acc: 0.9223]\n","[Epoch: 14]   [learning rate: 0.000300]   [loss avg: 4.5556]   [current loss: 0.0459]  [current acc: 0.9245]\n","[Epoch: 15]   [learning rate: 0.000300]   [loss avg: 4.3092]   [current loss: 0.1569]  [current acc: 0.9135]\n","[Epoch: 16]   [learning rate: 0.000300]   [loss avg: 4.0819]   [current loss: 0.0208]  [current acc: 0.9358]\n","[Epoch: 17]   [learning rate: 0.000300]   [loss avg: 3.8561]   [current loss: 0.0071]  [current acc: 0.9548]\n","[Epoch: 18]   [learning rate: 0.000300]   [loss avg: 3.6476]   [current loss: 0.0015]  [current acc: 0.9535]\n","[Epoch: 19]   [learning rate: 0.000300]   [loss avg: 3.4584]   [current loss: 0.0026]  [current acc: 0.9579]\n","[Epoch: 20]   [learning rate: 0.000240]   [loss avg: 3.2872]   [current loss: 0.0027]  [current acc: 0.9590]\n","[Epoch: 21]   [learning rate: 0.000240]   [loss avg: 3.1315]   [current loss: 0.0010]  [current acc: 0.9621]\n","[Epoch: 22]   [learning rate: 0.000240]   [loss avg: 2.9901]   [current loss: 0.0007]  [current acc: 0.9628]\n","[Epoch: 23]   [learning rate: 0.000240]   [loss avg: 2.8608]   [current loss: 0.0025]  [current acc: 0.9616]\n","[Epoch: 24]   [learning rate: 0.000240]   [loss avg: 2.7423]   [current loss: 0.0015]  [current acc: 0.9617]\n","[Epoch: 25]   [learning rate: 0.000240]   [loss avg: 2.6332]   [current loss: 0.0007]  [current acc: 0.9615]\n","[Epoch: 26]   [learning rate: 0.000240]   [loss avg: 2.5324]   [current loss: 0.0005]  [current acc: 0.9631]\n","[Epoch: 27]   [learning rate: 0.000240]   [loss avg: 2.4391]   [current loss: 0.0006]  [current acc: 0.9631]\n","[Epoch: 28]   [learning rate: 0.000240]   [loss avg: 2.3526]   [current loss: 0.0005]  [current acc: 0.9618]\n","[Epoch: 29]   [learning rate: 0.000240]   [loss avg: 2.2719]   [current loss: 0.0013]  [current acc: 0.9622]\n","[Epoch: 30]   [learning rate: 0.000240]   [loss avg: 2.1965]   [current loss: 0.0004]  [current acc: 0.9622]\n","[Epoch: 31]   [learning rate: 0.000240]   [loss avg: 2.1259]   [current loss: 0.0006]  [current acc: 0.9636]\n","[Epoch: 32]   [learning rate: 0.000240]   [loss avg: 2.0598]   [current loss: 0.0004]  [current acc: 0.9637]\n","[Epoch: 33]   [learning rate: 0.000240]   [loss avg: 1.9977]   [current loss: 0.0007]  [current acc: 0.9636]\n","[Epoch: 34]   [learning rate: 0.000240]   [loss avg: 1.9392]   [current loss: 0.0004]  [current acc: 0.9635]\n","[Epoch: 35]   [learning rate: 0.000240]   [loss avg: 1.8841]   [current loss: 0.0005]  [current acc: 0.9625]\n","[Epoch: 36]   [learning rate: 0.000240]   [loss avg: 1.8320]   [current loss: 0.0018]  [current acc: 0.9637]\n","[Epoch: 37]   [learning rate: 0.000240]   [loss avg: 1.7827]   [current loss: 0.0003]  [current acc: 0.9644]\n","[Epoch: 38]   [learning rate: 0.000240]   [loss avg: 1.7360]   [current loss: 0.0002]  [current acc: 0.9641]\n","[Epoch: 39]   [learning rate: 0.000240]   [loss avg: 1.6917]   [current loss: 0.0004]  [current acc: 0.9634]\n","[Epoch: 40]   [learning rate: 0.000192]   [loss avg: 1.6496]   [current loss: 0.0006]  [current acc: 0.9638]\n","[Epoch: 41]   [learning rate: 0.000192]   [loss avg: 1.6095]   [current loss: 0.0006]  [current acc: 0.9648]\n","[Epoch: 42]   [learning rate: 0.000192]   [loss avg: 1.5713]   [current loss: 0.0002]  [current acc: 0.9647]\n","[Epoch: 43]   [learning rate: 0.000192]   [loss avg: 1.5349]   [current loss: 0.0004]  [current acc: 0.9644]\n","[Epoch: 44]   [learning rate: 0.000192]   [loss avg: 1.5002]   [current loss: 0.0002]  [current acc: 0.9647]\n","[Epoch: 45]   [learning rate: 0.000192]   [loss avg: 1.4670]   [current loss: 0.0005]  [current acc: 0.9651]\n","[Epoch: 46]   [learning rate: 0.000192]   [loss avg: 1.4353]   [current loss: 0.0003]  [current acc: 0.9646]\n","[Epoch: 47]   [learning rate: 0.000192]   [loss avg: 1.4048]   [current loss: 0.0004]  [current acc: 0.9651]\n","[Epoch: 48]   [learning rate: 0.000192]   [loss avg: 1.3757]   [current loss: 0.0002]  [current acc: 0.9645]\n","[Epoch: 49]   [learning rate: 0.000192]   [loss avg: 1.3478]   [current loss: 0.0002]  [current acc: 0.9657]\n","[Epoch: 50]   [learning rate: 0.000192]   [loss avg: 1.3209]   [current loss: 0.0002]  [current acc: 0.9660]\n","[Epoch: 51]   [learning rate: 0.000192]   [loss avg: 1.2951]   [current loss: 0.0002]  [current acc: 0.9667]\n","[Epoch: 52]   [learning rate: 0.000192]   [loss avg: 1.2703]   [current loss: 0.0001]  [current acc: 0.9672]\n","[Epoch: 53]   [learning rate: 0.000192]   [loss avg: 1.2464]   [current loss: 0.0004]  [current acc: 0.9672]\n","[Epoch: 54]   [learning rate: 0.000192]   [loss avg: 1.2235]   [current loss: 0.0002]  [current acc: 0.9663]\n","[Epoch: 55]   [learning rate: 0.000192]   [loss avg: 1.2013]   [current loss: 0.0002]  [current acc: 0.9662]\n","[Epoch: 56]   [learning rate: 0.000192]   [loss avg: 1.1799]   [current loss: 0.0002]  [current acc: 0.9656]\n","[Epoch: 57]   [learning rate: 0.000192]   [loss avg: 1.1593]   [current loss: 0.0005]  [current acc: 0.9657]\n","[Epoch: 58]   [learning rate: 0.000192]   [loss avg: 1.1394]   [current loss: 0.0002]  [current acc: 0.9656]\n","[Epoch: 59]   [learning rate: 0.000192]   [loss avg: 1.1202]   [current loss: 0.0005]  [current acc: 0.9657]\n","[Epoch: 60]   [learning rate: 0.000154]   [loss avg: 1.1016]   [current loss: 0.0003]  [current acc: 0.9658]\n","[Epoch: 61]   [learning rate: 0.000154]   [loss avg: 1.0836]   [current loss: 0.0002]  [current acc: 0.9657]\n","[Epoch: 62]   [learning rate: 0.000154]   [loss avg: 1.0662]   [current loss: 0.0002]  [current acc: 0.9662]\n","[Epoch: 63]   [learning rate: 0.000154]   [loss avg: 1.0493]   [current loss: 0.0001]  [current acc: 0.9651]\n","[Epoch: 64]   [learning rate: 0.000154]   [loss avg: 1.0330]   [current loss: 0.0001]  [current acc: 0.9648]\n","[Epoch: 65]   [learning rate: 0.000154]   [loss avg: 1.0171]   [current loss: 0.0002]  [current acc: 0.9648]\n","[Epoch: 66]   [learning rate: 0.000154]   [loss avg: 1.0018]   [current loss: 0.0001]  [current acc: 0.9644]\n","[Epoch: 67]   [learning rate: 0.000154]   [loss avg: 0.9869]   [current loss: 0.0002]  [current acc: 0.9652]\n","[Epoch: 68]   [learning rate: 0.000154]   [loss avg: 0.9724]   [current loss: 0.0002]  [current acc: 0.9651]\n","[Epoch: 69]   [learning rate: 0.000154]   [loss avg: 0.9583]   [current loss: 0.0001]  [current acc: 0.9654]\n","[Epoch: 70]   [learning rate: 0.000154]   [loss avg: 0.9447]   [current loss: 0.0002]  [current acc: 0.9658]\n","[Epoch: 71]   [learning rate: 0.000154]   [loss avg: 0.9315]   [current loss: 0.0002]  [current acc: 0.9648]\n","[Epoch: 72]   [learning rate: 0.000154]   [loss avg: 0.9186]   [current loss: 0.0002]  [current acc: 0.9651]\n","[Epoch: 73]   [learning rate: 0.000154]   [loss avg: 0.9060]   [current loss: 0.0002]  [current acc: 0.9653]\n","[Epoch: 74]   [learning rate: 0.000154]   [loss avg: 0.8938]   [current loss: 0.0002]  [current acc: 0.9656]\n","[Epoch: 75]   [learning rate: 0.000154]   [loss avg: 0.8819]   [current loss: 0.0001]  [current acc: 0.9658]\n","[Epoch: 76]   [learning rate: 0.000154]   [loss avg: 0.8704]   [current loss: 0.0002]  [current acc: 0.9658]\n","[Epoch: 77]   [learning rate: 0.000154]   [loss avg: 0.8591]   [current loss: 0.0001]  [current acc: 0.9656]\n","[Epoch: 78]   [learning rate: 0.000154]   [loss avg: 0.8481]   [current loss: 0.0001]  [current acc: 0.9651]\n","[Epoch: 79]   [learning rate: 0.000154]   [loss avg: 0.8374]   [current loss: 0.0001]  [current acc: 0.9649]\n","[Epoch: 80]   [learning rate: 0.000123]   [loss avg: 0.8270]   [current loss: 0.0001]  [current acc: 0.9650]\n","[Epoch: 81]   [learning rate: 0.000123]   [loss avg: 0.8168]   [current loss: 0.0003]  [current acc: 0.9648]\n","[Epoch: 82]   [learning rate: 0.000123]   [loss avg: 0.8069]   [current loss: 0.0002]  [current acc: 0.9650]\n","[Epoch: 83]   [learning rate: 0.000123]   [loss avg: 0.7972]   [current loss: 0.0002]  [current acc: 0.9651]\n","[Epoch: 84]   [learning rate: 0.000123]   [loss avg: 0.7877]   [current loss: 0.0003]  [current acc: 0.9649]\n","[Epoch: 85]   [learning rate: 0.000123]   [loss avg: 0.7785]   [current loss: 0.0002]  [current acc: 0.9652]\n","[Epoch: 86]   [learning rate: 0.000123]   [loss avg: 0.7695]   [current loss: 0.0001]  [current acc: 0.9657]\n","[Epoch: 87]   [learning rate: 0.000123]   [loss avg: 0.7607]   [current loss: 0.0001]  [current acc: 0.9640]\n","[Epoch: 88]   [learning rate: 0.000123]   [loss avg: 0.7521]   [current loss: 0.0001]  [current acc: 0.9646]\n","[Epoch: 89]   [learning rate: 0.000123]   [loss avg: 0.7437]   [current loss: 0.0004]  [current acc: 0.9647]\n","[Epoch: 90]   [learning rate: 0.000123]   [loss avg: 0.7354]   [current loss: 0.0008]  [current acc: 0.9648]\n","[Epoch: 91]   [learning rate: 0.000123]   [loss avg: 0.7274]   [current loss: 0.0001]  [current acc: 0.9652]\n","[Epoch: 92]   [learning rate: 0.000123]   [loss avg: 0.7195]   [current loss: 0.0003]  [current acc: 0.9653]\n","[Epoch: 93]   [learning rate: 0.000123]   [loss avg: 0.7118]   [current loss: 0.0001]  [current acc: 0.9658]\n","[Epoch: 94]   [learning rate: 0.000123]   [loss avg: 0.7043]   [current loss: 0.0002]  [current acc: 0.9663]\n","[Epoch: 95]   [learning rate: 0.000123]   [loss avg: 0.6969]   [current loss: 0.0001]  [current acc: 0.9664]\n","[Epoch: 96]   [learning rate: 0.000123]   [loss avg: 0.6896]   [current loss: 0.0001]  [current acc: 0.9665]\n","[Epoch: 97]   [learning rate: 0.000123]   [loss avg: 0.6825]   [current loss: 0.0001]  [current acc: 0.9667]\n","[Epoch: 98]   [learning rate: 0.000123]   [loss avg: 0.6756]   [current loss: 0.0001]  [current acc: 0.9656]\n","[Epoch: 99]   [learning rate: 0.000123]   [loss avg: 0.6688]   [current loss: 0.0002]  [current acc: 0.9646]\n","[Epoch: 100]   [learning rate: 0.000098]   [loss avg: 0.6622]   [current loss: 0.0003]  [current acc: 0.9647]\n","Finished Training\n","Best Acc:0.9672\n"]}]},{"cell_type":"code","source":["net1.eval()   # 将模型设置为验证模式\n","# 测试最好的模型的结果\n","count = 0\n","# 模型测试\n","for inputs, _ in test_loader:\n","    inputs = inputs.to(device)\n","    outputs = net1(inputs)\n","    outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n","    if count == 0:\n","        y_pred_test =  outputs\n","        count = 1\n","    else:\n","        y_pred_test = np.concatenate( (y_pred_test, outputs) )\n","\n","# 生成分类报告\n","classification = classification_report(ytest, y_pred_test, digits=4)\n","print(classification)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6jQB8HGBVj1","executionInfo":{"status":"ok","timestamp":1673598330978,"user_tz":-480,"elapsed":12449,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"5cacde67-0f15-4826-e8bf-344ba78a230b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         0.0     0.9512    0.9512    0.9512        41\n","         1.0     0.9513    0.9276    0.9393      1285\n","         2.0     0.9468    0.9772    0.9618       747\n","         3.0     0.9539    0.9718    0.9628       213\n","         4.0     0.9836    0.9655    0.9745       435\n","         5.0     0.9909    0.9939    0.9924       657\n","         6.0     0.9600    0.9600    0.9600        25\n","         7.0     1.0000    1.0000    1.0000       430\n","         8.0     1.0000    0.8889    0.9412        18\n","         9.0     0.9682    0.9406    0.9542       875\n","        10.0     0.9617    0.9882    0.9748      2210\n","        11.0     0.9255    0.9307    0.9281       534\n","        12.0     1.0000    0.9676    0.9835       185\n","        13.0     0.9982    0.9965    0.9974      1139\n","        14.0     0.9611    0.9251    0.9427       347\n","        15.0     0.9231    0.8571    0.8889        84\n","\n","    accuracy                         0.9672      9225\n","   macro avg     0.9672    0.9526    0.9595      9225\n","weighted avg     0.9672    0.9672    0.9670      9225\n","\n"]}]},{"cell_type":"code","source":["# load the original image\n","X = sio.loadmat('/content/drive/MyDrive/AI data/hyperspetral image/Indian_pines_corrected.mat')['indian_pines_corrected']\n","y = sio.loadmat('/content/drive/MyDrive/AI data/hyperspetral image/Indian_pines_gt.mat')['indian_pines_gt']\n","\n","height = y.shape[0]\n","width = y.shape[1]\n","print(\"height:\",height)\n","print(\"width:\",width)\n","\n","X = applyPCA(X, numComponents= pca_components)\n","X = padWithZeros(X, 21//2)\n","\n","# 逐像素预测类别\n","outputs = np.zeros((height,width))\n","for i in range(height):\n","    for j in range(width):\n","        if int(y[i,j]) == 0:\n","            continue\n","        else :\n","            image_patch = X[i:i+patch_size, j:j+patch_size, :]\n","            #print(image_patch.shape)\n","            image_patch = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2])\n","            #print(image_patch.shape)\n","            X_test_image = torch.FloatTensor(image_patch.transpose(0, 3, 1, 2)).to(device)                                   \n","            prediction = net1(X_test_image)\n","            prediction = np.argmax(prediction.detach().cpu().numpy(), axis=1)\n","            outputs[i][j] = prediction+1\n","    if i % 20 == 0:\n","        print('... ... row ', i, ' handling ... ...')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1q2xVV0hBp7t","executionInfo":{"status":"ok","timestamp":1673598573587,"user_tz":-480,"elapsed":242615,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"23097ec2-46e8-4ab3-c4ff-82433253231b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["height: 145\n","width: 145\n","... ... row  0  handling ... ...\n","... ... row  20  handling ... ...\n","... ... row  40  handling ... ...\n","... ... row  60  handling ... ...\n","... ... row  80  handling ... ...\n","... ... row  100  handling ... ...\n","... ... row  120  handling ... ...\n","... ... row  140  handling ... ...\n"]}]},{"cell_type":"code","source":["hight=145\n","width=145\n","num_correct = 0\n","num_false = 0\n","for i in range(hight):\n","  for j in range(width):\n","    if outputs[i,j] == y[i,j]:\n","      num_correct += 1\n","    if outputs[i,j] != y[i,j]:\n","      num_false += 1\n","print(\"预测正确的像素个数为：{} | 预测错误的个数为：{} | 预测准确率：{}%\".format(int(num_correct),int(num_false),float((num_correct/(145*145)*100))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFo8XnEiB3Xs","executionInfo":{"status":"ok","timestamp":1673598573588,"user_tz":-480,"elapsed":29,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"9466149c-23aa-46c9-d05c-e11440da63ac"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["预测正确的像素个数为：20713 | 预测错误的个数为：312 | 预测准确率：98.51605231866826%\n"]}]}]}