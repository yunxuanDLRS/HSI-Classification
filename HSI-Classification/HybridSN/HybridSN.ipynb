{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1M2H_yGnYvAbVX31ojIZ2n9GrdncGJy8u","authorship_tag":"ABX9TyPHiUSg0ihpDJU23eRwF4rZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install spectral"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VajHu7spaCir","executionInfo":{"status":"ok","timestamp":1663249884841,"user_tz":-480,"elapsed":4891,"user":{"displayName":"ma yunxuan","userId":"01991168991301125311"}},"outputId":"6928cbba-57f1-498b-de29-df12d829ba26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spectral\n","  Downloading spectral-0.23-py3-none-any.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 34.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spectral) (1.21.6)\n","Installing collected packages: spectral\n","Successfully installed spectral-0.23\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ath-i3IVF-H","executionInfo":{"status":"ok","timestamp":1663250182362,"user_tz":-480,"elapsed":9402,"user":{"displayName":"ma yunxuan","userId":"01991168991301125311"}},"outputId":"cd2c6c79-ec24-4854-e0df-71082d0b305c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-09-15 13:56:12--  http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n","Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n","Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat [following]\n","--2022-09-15 13:56:12--  https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n","Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5953527 (5.7M)\n","Saving to: ‘Indian_pines_corrected.mat’\n","\n","Indian_pines_correc 100%[===================>]   5.68M  1.58MB/s    in 3.6s    \n","\n","2022-09-15 13:56:17 (1.58 MB/s) - ‘Indian_pines_corrected.mat’ saved [5953527/5953527]\n","\n","--2022-09-15 13:56:17--  http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n","Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n","Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat [following]\n","--2022-09-15 13:56:17--  https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n","Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1125 (1.1K)\n","Saving to: ‘Indian_pines_gt.mat’\n","\n","Indian_pines_gt.mat 100%[===================>]   1.10K  --.-KB/s    in 0s      \n","\n","2022-09-15 13:56:18 (70.5 MB/s) - ‘Indian_pines_gt.mat’ saved [1125/1125]\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spectral in /usr/local/lib/python3.7/dist-packages (0.23)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spectral) (1.21.6)\n"]}],"source":["#数据准备\n","\n","! wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n","! wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n","! pip install spectral\n"]},{"cell_type":"code","source":["##引入基本函数库\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io as sio\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n","import spectral\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n"],"metadata":{"id":"_3ogkKLhadv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##定义 HybridSN 类\n","\n","class_num = 16\n","\n","class HybridSN(nn.Module):\n","\n","  def __init__(self, num_classes=16):\n","    super(HybridSN, self).__init__()\n","    # conv1：（1, 30, 25, 25）， 8个 7x3x3 的卷积核 ==>（8, 24, 23, 23）\n","    self.conv1 = nn.Conv3d(1, 8, (7, 3, 3))\n","    # conv2：（8, 24, 23, 23）， 16个 5x3x3 的卷积核 ==>（16, 20, 21, 21）\n","    self.conv2 = nn.Conv3d(8, 16, (5, 3, 3))\n","    # conv3：（16, 20, 21, 21），32个 3x3x3 的卷积核 ==>（32, 18, 19, 19）\n","    self.conv3 = nn.Conv3d(16, 32, (3, 3, 3))\n","    # conv3_2d （576, 19, 19），64个 3x3 的卷积核 ==>（（64, 17, 17）\n","    self.conv3_2d = nn.Conv2d(576, 64, (3,3))\n","    # 全连接层（256个节点）\n","    self.dense1 =  nn.Linear(18496,256)\n","    # 全连接层（128个节点）\n","    self.dense2 =  nn.Linear(256,128)\n","    # 最终输出层(16个节点)\n","    self.out = nn.Linear(128, num_classes)\n","    #  Dropout（0.4)\n","    self.drop = nn.Dropout(p=0.4)\n","\n","\t######################\n","\t#这里是优化的方向（一会还会添加注意力机制）\n","\t######################\n","    #软最大化\n","    #self.soft = nn.LogSoftmax(dim=1)\n","    self.soft = nn.Softmax(dim=1)\n","\n","    #加入BN归一化数据\n","    self.bn1=nn.BatchNorm3d(8)\n","    self.bn2=nn.BatchNorm3d(16)\n","    self.bn3=nn.BatchNorm3d(32)\n","    self.bn4=nn.BatchNorm2d(64)\n","    \n","    # 激活函数ReLU\n","    self.relu = nn.ReLU()\n","\t#####################\n","\t#####################\n","\n","  #定义完了各个模块，记得在这里调用执行：\n","  def forward(self, x):\n","\n","    out = self.relu(self.conv1(x))\n","    #out = self.bn1(out)#BN层\n","\n","    out = self.relu(self.conv2(out))\n","    #out = self.bn2(out)#BN层\n","    \n","    out = self.relu(self.conv3(out))\n","    #out = self.bn3(out)#BN层\n","\n","    out = out.view(-1, out.shape[1] * out.shape[2], out.shape[3], out.shape[4])# 进行二维卷积，因此把前面的 32*18 reshape 一下，得到 （576, 19, 19）\n","    #out = self.attention(out)#调用注意力机制\n","    out = self.relu(self.conv3_2d(out))\n","    #out = self.bn4(out)#BN层\n","    \n","    # flatten 操作，变为 18496 维的向量，\n","    out = out.view(out.size(0), -1)\n","    out = self.dense1(out)\n","    out = self.drop(out)\n","    out = self.dense2(out)\n","    out = self.drop(out)\n","    out = self.out(out)\n","    out = self.soft(out)\n","    return out\n","\n"],"metadata":{"id":"ysEz1pqja5qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 对高光谱数据 X 应用 PCA 变换\n","def applyPCA(X, numComponents):\n","    newX = np.reshape(X, (-1, X.shape[2]))\n","    pca = PCA(n_components=numComponents, whiten=True)\n","    newX = pca.fit_transform(newX)\n","    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n","    return newX\n","\n","# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\n","def padWithZeros(X, margin=2):\n","    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n","    x_offset = margin\n","    y_offset = margin\n","    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n","    return newX\n","\n","# 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\n","def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n","    # 给 X 做 padding\n","    margin = int((windowSize - 1) / 2)\n","    zeroPaddedX = padWithZeros(X, margin=margin)\n","    # split patches\n","    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n","    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n","    patchIndex = 0\n","    for r in range(margin, zeroPaddedX.shape[0] - margin):\n","        for c in range(margin, zeroPaddedX.shape[1] - margin):\n","            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n","            patchesData[patchIndex, :, :, :] = patch\n","            patchesLabels[patchIndex] = y[r-margin, c-margin]\n","            patchIndex = patchIndex + 1\n","    if removeZeroLabels:\n","        patchesData = patchesData[patchesLabels>0,:,:,:]\n","        patchesLabels = patchesLabels[patchesLabels>0]\n","        patchesLabels -= 1\n","    return patchesData, patchesLabels\n","\n","def splitTrainTestSet(X, y, testRatio, randomState=345):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n","    return X_train, X_test, y_train, y_test\n"],"metadata":{"id":"2a-9OEvnbcRw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 地物类别\n","class_num = 16\n","X = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n","y = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n","\n","# 用于测试样本的比例\n","test_ratio = 0.90\n","# 每个像素周围提取 patch 的尺寸\n","patch_size = 25\n","# 使用 PCA 降维，得到主成分的数量\n","pca_components = 30\n","\n","print('Hyperspectral data shape: ', X.shape)\n","print('Label shape: ', y.shape)\n","\n","print('\\n... ... PCA tranformation ... ...')\n","X_pca = applyPCA(X, numComponents=pca_components)\n","print('Data shape after PCA: ', X_pca.shape)\n","\n","print('\\n... ... create data cubes ... ...')\n","X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)\n","print('Data cube X shape: ', X_pca.shape)\n","print('Data cube y shape: ', y.shape)\n","\n","print('\\n... ... create train & test data ... ...')\n","Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X_pca, y, test_ratio)\n","print('Xtrain shape: ', Xtrain.shape)\n","print('Xtest  shape: ', Xtest.shape)\n","\n","# 改变 Xtrain, Ytrain 的形状，以符合 keras 的要求\n","Xtrain = Xtrain.reshape(-1, patch_size, patch_size, pca_components, 1)\n","Xtest  = Xtest.reshape(-1, patch_size, patch_size, pca_components, 1)\n","print('before transpose: Xtrain shape: ', Xtrain.shape) \n","print('before transpose: Xtest  shape: ', Xtest.shape) \n","\n","# 为了适应 pytorch 结构，数据要做 transpose\n","Xtrain = Xtrain.transpose(0, 4, 3, 1, 2)\n","Xtest  = Xtest.transpose(0, 4, 3, 1, 2)\n","print('after transpose: Xtrain shape: ', Xtrain.shape) \n","print('after transpose: Xtest  shape: ', Xtest.shape) \n","\n","\n","\"\"\" Training dataset\"\"\"\n","class TrainDS(torch.utils.data.Dataset): \n","    def __init__(self):\n","        self.len = Xtrain.shape[0]\n","        self.x_data = torch.FloatTensor(Xtrain)\n","        self.y_data = torch.LongTensor(ytrain)        \n","    def __getitem__(self, index):\n","        # 根据索引返回数据和对应的标签\n","        return self.x_data[index], self.y_data[index]\n","    def __len__(self): \n","        # 返回文件数据的数目\n","        return self.len\n","\n","\"\"\" Testing dataset\"\"\"\n","class TestDS(torch.utils.data.Dataset): \n","    def __init__(self):\n","        self.len = Xtest.shape[0]\n","        self.x_data = torch.FloatTensor(Xtest)\n","        self.y_data = torch.LongTensor(ytest)\n","    def __getitem__(self, index):\n","        # 根据索引返回数据和对应的标签\n","        return self.x_data[index], self.y_data[index]\n","    def __len__(self): \n","        # 返回文件数据的数目\n","        return self.len\n","\n","# 创建 trainloader 和 testloader\n","trainset = TrainDS()\n","testset  = TestDS()\n","train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=128, shuffle=True, num_workers=2)\n","test_loader  = torch.utils.data.DataLoader(dataset=testset,  batch_size=128, shuffle=False, num_workers=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhjCpwFpbiGZ","outputId":"9f88d306-2f0d-45ac-d51d-74ef306e752e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperspectral data shape:  (145, 145, 200)\n","Label shape:  (145, 145)\n","\n","... ... PCA tranformation ... ...\n","Data shape after PCA:  (145, 145, 30)\n","\n","... ... create data cubes ... ...\n"]}]},{"cell_type":"code","source":["# 使用GPU训练，可以在菜单 \"代码执行工具\" -> \"更改运行时类型\" 里进行设置\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 网络放到GPU上\n","net = HybridSN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001)#设置学习率\n","\n","# 计时\n","import time\n","time_start=time.time()\n","\n","# 开始训练\n","total_loss = 0\n","for epoch in range(100):\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        # 优化器梯度归零\n","        optimizer.zero_grad()\n","        # 正向传播 +　反向传播 + 优化 \n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n","    \n","time_end=time.time()\n","print('totally cost',time_end-time_start)\n","print('Finished Training')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YgQjfy0cb7p","executionInfo":{"status":"ok","timestamp":1663250524009,"user_tz":-480,"elapsed":201813,"user":{"displayName":"ma yunxuan","userId":"01991168991301125311"}},"outputId":"2129ba5c-e7e4-40d3-e8b2-5561d737d855"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch: 1]   [loss avg: 21.2862]   [current loss: 2.6090]\n","[Epoch: 2]   [loss avg: 21.1845]   [current loss: 2.6246]\n","[Epoch: 3]   [loss avg: 21.1505]   [current loss: 2.5855]\n","[Epoch: 4]   [loss avg: 21.1336]   [current loss: 2.5933]\n","[Epoch: 5]   [loss avg: 21.1234]   [current loss: 2.6246]\n","[Epoch: 6]   [loss avg: 21.1166]   [current loss: 2.6715]\n","[Epoch: 7]   [loss avg: 21.1118]   [current loss: 2.6324]\n","[Epoch: 8]   [loss avg: 21.1081]   [current loss: 2.6324]\n","[Epoch: 9]   [loss avg: 21.1053]   [current loss: 2.6090]\n","[Epoch: 10]   [loss avg: 21.1031]   [current loss: 2.6246]\n","[Epoch: 11]   [loss avg: 21.1012]   [current loss: 2.6480]\n","[Epoch: 12]   [loss avg: 21.0997]   [current loss: 2.5933]\n","[Epoch: 13]   [loss avg: 21.0984]   [current loss: 2.6480]\n","[Epoch: 14]   [loss avg: 21.0972]   [current loss: 2.6871]\n","[Epoch: 15]   [loss avg: 21.0963]   [current loss: 2.6715]\n","[Epoch: 16]   [loss avg: 21.0954]   [current loss: 2.6168]\n","[Epoch: 17]   [loss avg: 21.0947]   [current loss: 2.6715]\n","[Epoch: 18]   [loss avg: 21.0940]   [current loss: 2.5933]\n","[Epoch: 19]   [loss avg: 21.0934]   [current loss: 2.6090]\n","[Epoch: 20]   [loss avg: 21.0929]   [current loss: 2.6324]\n","[Epoch: 21]   [loss avg: 21.0924]   [current loss: 2.6402]\n","[Epoch: 22]   [loss avg: 21.0920]   [current loss: 2.6324]\n","[Epoch: 23]   [loss avg: 21.0916]   [current loss: 2.6558]\n","[Epoch: 24]   [loss avg: 21.0912]   [current loss: 2.6480]\n","[Epoch: 25]   [loss avg: 21.0908]   [current loss: 2.6715]\n","[Epoch: 26]   [loss avg: 21.0905]   [current loss: 2.6246]\n","[Epoch: 27]   [loss avg: 21.0902]   [current loss: 2.6637]\n","[Epoch: 28]   [loss avg: 21.0900]   [current loss: 2.6558]\n","[Epoch: 29]   [loss avg: 21.0897]   [current loss: 2.6168]\n","[Epoch: 30]   [loss avg: 21.0895]   [current loss: 2.6793]\n","[Epoch: 31]   [loss avg: 21.0893]   [current loss: 2.6168]\n","[Epoch: 32]   [loss avg: 21.0891]   [current loss: 2.6324]\n","[Epoch: 33]   [loss avg: 21.0889]   [current loss: 2.6715]\n","[Epoch: 34]   [loss avg: 21.0887]   [current loss: 2.5699]\n","[Epoch: 35]   [loss avg: 21.0885]   [current loss: 2.6637]\n","[Epoch: 36]   [loss avg: 21.0884]   [current loss: 2.6324]\n","[Epoch: 37]   [loss avg: 21.0882]   [current loss: 2.6402]\n","[Epoch: 38]   [loss avg: 21.0881]   [current loss: 2.6324]\n","[Epoch: 39]   [loss avg: 21.0879]   [current loss: 2.6324]\n","[Epoch: 40]   [loss avg: 21.0878]   [current loss: 2.5543]\n","[Epoch: 41]   [loss avg: 21.0877]   [current loss: 2.6246]\n","[Epoch: 42]   [loss avg: 21.0876]   [current loss: 2.6168]\n","[Epoch: 43]   [loss avg: 21.0874]   [current loss: 2.6012]\n","[Epoch: 44]   [loss avg: 21.0873]   [current loss: 2.7027]\n","[Epoch: 45]   [loss avg: 21.0872]   [current loss: 2.6324]\n","[Epoch: 46]   [loss avg: 21.0871]   [current loss: 2.6324]\n","[Epoch: 47]   [loss avg: 21.0870]   [current loss: 2.6637]\n","[Epoch: 48]   [loss avg: 21.0869]   [current loss: 2.7262]\n","[Epoch: 49]   [loss avg: 21.0869]   [current loss: 2.5308]\n","[Epoch: 50]   [loss avg: 21.0868]   [current loss: 2.6949]\n","[Epoch: 51]   [loss avg: 21.0867]   [current loss: 2.7183]\n","[Epoch: 52]   [loss avg: 21.0866]   [current loss: 2.6480]\n","[Epoch: 53]   [loss avg: 21.0865]   [current loss: 2.6168]\n","[Epoch: 54]   [loss avg: 21.0865]   [current loss: 2.6715]\n","[Epoch: 55]   [loss avg: 21.0864]   [current loss: 2.6480]\n","[Epoch: 56]   [loss avg: 21.0863]   [current loss: 2.7183]\n","[Epoch: 57]   [loss avg: 21.0863]   [current loss: 2.6324]\n","[Epoch: 58]   [loss avg: 21.0862]   [current loss: 2.6637]\n","[Epoch: 59]   [loss avg: 21.0862]   [current loss: 2.6012]\n","[Epoch: 60]   [loss avg: 21.0861]   [current loss: 2.6637]\n","[Epoch: 61]   [loss avg: 21.0860]   [current loss: 2.6324]\n","[Epoch: 62]   [loss avg: 21.0860]   [current loss: 2.6715]\n","[Epoch: 63]   [loss avg: 21.0859]   [current loss: 2.5777]\n","[Epoch: 64]   [loss avg: 21.0859]   [current loss: 2.6637]\n","[Epoch: 65]   [loss avg: 21.0858]   [current loss: 2.6402]\n","[Epoch: 66]   [loss avg: 21.0858]   [current loss: 2.6012]\n","[Epoch: 67]   [loss avg: 21.0857]   [current loss: 2.5933]\n","[Epoch: 68]   [loss avg: 21.0857]   [current loss: 2.6949]\n","[Epoch: 69]   [loss avg: 21.0857]   [current loss: 2.6090]\n","[Epoch: 70]   [loss avg: 21.0856]   [current loss: 2.6558]\n","[Epoch: 71]   [loss avg: 21.0856]   [current loss: 2.6090]\n","[Epoch: 72]   [loss avg: 21.0855]   [current loss: 2.6012]\n","[Epoch: 73]   [loss avg: 21.0855]   [current loss: 2.6637]\n","[Epoch: 74]   [loss avg: 21.0855]   [current loss: 2.6558]\n","[Epoch: 75]   [loss avg: 21.0854]   [current loss: 2.6324]\n","[Epoch: 76]   [loss avg: 21.0854]   [current loss: 2.6012]\n","[Epoch: 77]   [loss avg: 21.0854]   [current loss: 2.5855]\n","[Epoch: 78]   [loss avg: 21.0853]   [current loss: 2.6871]\n","[Epoch: 79]   [loss avg: 21.0853]   [current loss: 2.7105]\n","[Epoch: 80]   [loss avg: 21.0853]   [current loss: 2.6558]\n","[Epoch: 81]   [loss avg: 21.0852]   [current loss: 2.6168]\n","[Epoch: 82]   [loss avg: 21.0852]   [current loss: 2.6246]\n","[Epoch: 83]   [loss avg: 21.0852]   [current loss: 2.5855]\n","[Epoch: 84]   [loss avg: 21.0851]   [current loss: 2.6324]\n","[Epoch: 85]   [loss avg: 21.0851]   [current loss: 2.6324]\n","[Epoch: 86]   [loss avg: 21.0851]   [current loss: 2.6012]\n","[Epoch: 87]   [loss avg: 21.0850]   [current loss: 2.6012]\n","[Epoch: 88]   [loss avg: 21.0850]   [current loss: 2.6402]\n","[Epoch: 89]   [loss avg: 21.0850]   [current loss: 2.6402]\n","[Epoch: 90]   [loss avg: 21.0850]   [current loss: 2.5933]\n","[Epoch: 91]   [loss avg: 21.0849]   [current loss: 2.6637]\n","[Epoch: 92]   [loss avg: 21.0849]   [current loss: 2.6715]\n","[Epoch: 93]   [loss avg: 21.0849]   [current loss: 2.5933]\n","[Epoch: 94]   [loss avg: 21.0849]   [current loss: 2.6558]\n","[Epoch: 95]   [loss avg: 21.0849]   [current loss: 2.6246]\n","[Epoch: 96]   [loss avg: 21.0848]   [current loss: 2.6012]\n","[Epoch: 97]   [loss avg: 21.0848]   [current loss: 2.6480]\n","[Epoch: 98]   [loss avg: 21.0848]   [current loss: 2.6090]\n","[Epoch: 99]   [loss avg: 21.0848]   [current loss: 2.5855]\n","[Epoch: 100]   [loss avg: 21.0847]   [current loss: 2.6012]\n","totally cost 196.90327048301697\n","Finished Training\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9teqNyKfenrX"},"execution_count":null,"outputs":[]}]}