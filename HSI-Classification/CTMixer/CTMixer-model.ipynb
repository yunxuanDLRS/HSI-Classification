{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJRFa+8UkggoRQXatIdedN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Convolution Transformer Mixer for Hyperspectral Image Classification**"],"metadata":{"id":"t7vB2uNhw1KA"}},{"cell_type":"code","source":["!pip install einops\n","!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6eKMvSMPhN1","executionInfo":{"status":"ok","timestamp":1682180768021,"user_tz":-480,"elapsed":19256,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"0ab9ba03-8664-460a-a098-82a8605ce3d0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.2\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"i5hkZNbWPNU4","executionInfo":{"status":"ok","timestamp":1682180775075,"user_tz":-480,"elapsed":7059,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"outputs":[],"source":["import einops\n","import torch\n","import math\n","import torch.nn as nn\n","import numpy as np\n","from torch.autograd import Variable\n","from einops.layers.torch import Rearrange\n","\n","\n","class PatchEmbeddings(nn.Module):\n","    \"\"\"\n","    Module that extracts patches and projects them\n","    \"\"\"\n","    def __init__(self, patch_size: int, patch_dim: int, emb_dim: int):\n","        super().__init__()\n","        self.patchify = Rearrange(\n","            \"b c (h p1) (w p2) -> b (h w) c p1 p2\",\n","            # \"b c (h p1) (w p2) -> b (p1 p2) c h w\",\n","            p1=patch_size, p2=patch_size)\n","\n","        self.flatten = nn.Flatten(start_dim=2)\n","        self.proj = nn.Linear(in_features=patch_dim, out_features=emb_dim)\n","        #print(patch_dim)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # Rearrange into patches\n","        x = self.patchify(x)\n","        # Flatten patches into individual vectors\n","        x = self.flatten(x)\n","        # Project to higher dim\n","        x = self.proj(x)\n","        return x\n","\n","\n","class CLSToken(nn.Module):\n","    \"\"\"\n","    Prepend cls token to each embedding\n","    \"\"\"\n","    def __init__(self, dim: int):\n","        super().__init__()\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        b = x.shape[0]\n","        cls_tokens = self.cls_token.repeat(b, 1, 1)\n","        x = torch.cat([cls_tokens, x], dim=1)\n","        return x\n","\n","\n","class PositionalEmbeddings(nn.Module):\n","    \"\"\"\n","    Learned positional embeddings\n","    \"\"\"\n","    def __init__(self, num_pos: int, dim: int):\n","        super().__init__()\n","        self.pos = nn.Parameter(torch.randn(num_pos, dim))\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return x + self.pos\n","\n","import einops\n","import torch\n","import math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from einops.layers.torch import Rearrange\n","\n","\n","class Attention_Conv(nn.Module):\n","\n","    def __init__(self, dim: int, head_dim: int, num_heads: int, num_patch: int, patch_size: int):\n","        super().__init__()\n","\n","        self.dim = dim\n","        self.head_dim = head_dim\n","        self.num_patch = num_patch\n","        self.patch_size = patch_size\n","        self.num_heads = num_heads\n","        self.inner_dim = head_dim * num_heads\n","        self.scale = head_dim ** -0.5\n","        self.attn = nn.Softmax(dim=-1)\n","        self.act = nn.ReLU(inplace=True)\n","        self.bn = nn.BatchNorm2d(dim)\n","        self.qkv = nn.Conv2d(dim, self.inner_dim * 3, kernel_size=1, padding=0, groups=dim, bias=False)\n","        self.avgpool=nn.AdaptiveAvgPool1d(dim)\n","\n","        self.qs = nn.Conv2d(dim, dim, kernel_size=(1, 3), padding=(0, 1), groups=dim, bias=False)\n","        self.ks = nn.Conv2d(dim, dim, kernel_size=(3, 1), padding=(1, 0), groups=dim, bias=False)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        b, n, d = x.shape\n","        x = x.contiguous().view(b, self.dim, self.num_patch, self.num_patch)\n","\n","        qkv = self.qkv(self.act(self.bn(x)))\n","        qkv = qkv.contiguous().view(b, self.num_patch*self.num_patch, self.inner_dim * 3)\n","        qkv = qkv.chunk(3, dim=-1)\n","        q, k, v = map(lambda t: einops.rearrange(t, \"b n (h d) -> b h n d\", h=self.num_heads), qkv)\n","\n","        q_1 = q[:, 0:self.num_heads//2, :, :]\n","        k_1 = k[:, 0:self.num_heads//2, :, :]\n","        v_1 = v[:, 0:self.num_heads//2, :, :]\n","        q_2 = q[:, self.num_heads//2:self.num_heads, :, :].reshape(b, -1, int(math.sqrt(n)), int(math.sqrt(n)))\n","        k_2 = k[:, self.num_heads//2:self.num_heads, :, :].reshape(b, -1, int(math.sqrt(n)), int(math.sqrt(n)))\n","        v_2 = v[:, self.num_heads//2:self.num_heads, :, :].reshape(b, -1, int(math.sqrt(n)), int(math.sqrt(n)))\n","\n","        q_2 = self.qs(q_2)\n","        k_2 = self.ks(k_2)\n","        res_2 = (q_2 + k_2 + v_2).reshape(b, n, -1)\n","\n","        scores = torch.einsum(\"b h i d, b h j d -> b h i j\", q_1, k_1)\n","        scores = scores * self.scale\n","        attn = self.attn(scores)\n","        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn, v_1)\n","        out = einops.rearrange(out, \"b h n d -> b n (h d)\")\n","        res = torch.cat([out, res_2], axis=2)\n","        out = self.avgpool(res)\n","        return out\n","    \n","\n","class FeedForward_Conv(nn.Module):\n","\n","    def __init__(self, dim: int, hidden_dim: int, num_patch: int, patch_size: int):\n","        super().__init__()\n","        self.dim = dim\n","        self.num_patch = num_patch\n","        self.patch_size = patch_size\n","        self.conv1 = nn.Sequential(\n","            nn.BatchNorm2d(dim), nn.GELU(), \n","            nn.Conv2d(dim, 64, kernel_size=1, padding=0, bias=False))\n","\n","        self.conv2 = nn.Sequential(\n","            nn.BatchNorm2d(64), nn.GELU(), \n","            nn.Conv2d(64, 64, kernel_size=3, padding=1, groups=64, bias=False))\n","        \n","        self.conv3 = nn.Sequential(\n","            nn.BatchNorm2d(64), nn.GELU(), \n","            nn.Conv2d(64, dim, kernel_size=1, padding=0, bias=False))\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        b, hw, dim = x.shape     # [bs, num_seq, dim]\n","        x_reshape = x.contiguous().view(b, self.dim, self.num_patch, self.num_patch)\n","        out1 = self.conv1(x_reshape)\n","        out2 = self.conv2(out1)\n","        out3 = self.conv3(out2) + x_reshape\n","        result = out3.contiguous().view(b, self.num_patch * self.num_patch, self.dim)\n","\n","        return result\n","\n","\n","class transformer(nn.Module):\n","\n","    def __init__(self, dim: int, num_layers: int, num_heads: int, head_dim: int, hidden_dim: int, num_patch: int, patch_size: int):\n","        super().__init__()\n","        self.layers = nn.ModuleList()\n","        for _ in range(num_layers):\n","            layer = [\n","                nn.Sequential(nn.LayerNorm(dim), Attention_Conv(dim, head_dim, num_heads, num_patch, patch_size)),\n","                nn.Sequential(nn.LayerNorm(dim), FeedForward_Conv(dim, hidden_dim, num_patch, patch_size))\n","            ]\n","            self.layers.append(nn.ModuleList(layer))\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","import math\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","from torch.nn import init\n","from torchinfo import summary\n","import torch.nn.functional as F\n","\n","\n","class Pooling(nn.Module):\n","    \"\"\"\n","    @article{ref-vit,\n","\ttitle={An image is worth 16x16 words: Transformers for image recognition at scale},\n","\tauthor={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, \n","            Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},\n","\tjournal={arXiv preprint arXiv:2010.11929},\n","\tyear={2020}\n","    }\n","    \"\"\"\n","    def __init__(self, pool: str = \"mean\"):\n","        super().__init__()\n","        if pool not in [\"mean\", \"cls\"]:\n","            raise ValueError(\"pool must be one of {mean, cls}\")\n","\n","        self.pool_fn = self.mean_pool if pool == \"mean\" else self.cls_pool\n","\n","    def mean_pool(self, x: torch.Tensor) -> torch.Tensor:\n","        return x.mean(dim=1)\n","\n","    def cls_pool(self, x: torch.Tensor) -> torch.Tensor:\n","        return x[:, 0]\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.pool_fn(x)\n","\n","\n","class Classifier(nn.Module):\n","\n","    def __init__(self, dim: int, num_classes: int):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(in_features=dim, out_features=num_classes)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.model(x)\n","\n","\n","class Res2(nn.Module):  \n","    \"\"\"\n","    @article{ref-sprn,\n","\ttitle={Spectral partitioning residual network with spatial attention mechanism for hyperspectral image classification},\n","\tauthor={Zhang, Xiangrong and Shang, Shouwang and Tang, Xu and Feng, Jie and Jiao, Licheng},\n","\tjournal={IEEE Trans. Geosci. Remote Sens.},\n","\tvolume={60},\n","\tpages={1--14},\n","\tyear={2021},\n","\tpublisher={IEEE}\n","    }\n","    \"\"\"\n","    def __init__(self, in_channels, inter_channels, kernel_size, padding=0):\n","        super(Res2, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, inter_channels, kernel_size=kernel_size, padding=padding)\n","        self.bn1 = nn.BatchNorm2d(inter_channels)\n","        self.conv2 = nn.Conv2d(inter_channels, in_channels, kernel_size=kernel_size, padding=padding)\n","        self.bn2 = nn.BatchNorm2d(in_channels)\n","\n","    def forward(self, X):\n","        X = F.relu(self.bn1(self.conv1(X)))\n","        X = self.bn2(self.conv2(X))\n","        return X\n","\n","\n","class Res(nn.Module):  \n","    def __init__(self, in_channels, kernel_size, padding, groups_s):\n","        super(Res, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=groups_s)\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","\n","        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=groups_s)\n","        self.bn2 = nn.BatchNorm2d(in_channels)\n","\n","        self.res2 = Res2(in_channels, 32, kernel_size=kernel_size, padding=padding)\n","\n","    def forward(self, X):\n","        Y = F.relu(self.bn1(self.conv1(X)))\n","        Y = self.bn2(self.conv2(Y))\n","        Z = self.res2(X)\n","        return F.relu(X + Y + Z)\n","\n","\n","class CTMixer(nn.Module):\n","\n","    def __init__(self, channels, num_classes, image_size, datasetname, num_layers: int=1, num_heads: int=4, \n","                 patch_size: int = 1, emb_dim: int = 128, head_dim = 64, hidden_dim: int = 64, pool: str = \"mean\"):\n","        super().__init__()\n","        self.emb_dim = emb_dim\n","\n","        self.hidden_dim = hidden_dim\n","        self.channels = channels\n","        self.image_size = image_size\n","        self.num_patches = (image_size // patch_size) ** 2\n","        self.num_patch = int(math.sqrt(self.num_patches))\n","        self.act = nn.ReLU(inplace=True)\n","        patch_dim = channels * patch_size ** 2\n","\n","        # Conv Preprocessing Module (Ref-SPRN)\n","        if datasetname == 'IndianPines':\n","            groups = 11\n","            groups_width = 37\n","        elif datasetname == 'PaviaU':\n","            groups = 5\n","            groups_width = 64\n","        elif datasetname == 'Salinas':\n","            groups = 11\n","            groups_width = 37\n","        elif datasetname == 'Houston':\n","            groups = 5\n","            groups_width = 64\n","        else:\n","            groups = 11\n","            groups_width = 37\n","        new_bands = math.ceil(channels/groups) * groups\n","        patch_dim = (groups*groups_width) * patch_size ** 2\n","        pad_size = new_bands - channels\n","        self.pad = nn.ReplicationPad3d((0, 0, 0, 0, 0, pad_size))\n","        self.conv_1 = nn.Conv2d(new_bands, groups*groups_width, (1, 1), groups=groups)\n","        self.bn_1 = nn.BatchNorm2d(groups*groups_width)\n","        self.res0 = Res(groups*groups_width, (3, 3), (1, 1), groups_s=groups)\n","\n","        # # Dual Residual Block (Ref-RDACN (mine))\n","        self.bn1 = nn.BatchNorm2d(emb_dim)\n","        self.conv1 = nn.Conv2d(emb_dim, 64, kernel_size=1, padding=0)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, groups=64)\n","        self.bn3 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, emb_dim, kernel_size=1, padding=0)\n","\n","        # Vision Transformer\n","        self.patch_embeddings = PatchEmbeddings(patch_size=patch_size, patch_dim=patch_dim, emb_dim=emb_dim)\n","        self.pos_embeddings = PositionalEmbeddings(num_pos=self.num_patches, dim=emb_dim)\n","        self.transformer = transformer(dim=emb_dim, num_layers=num_layers, num_heads=num_heads, \n","                                        head_dim=head_dim, hidden_dim=hidden_dim, num_patch=self.num_patch, patch_size=patch_size)\n","        self.dropout = nn.Dropout(0.5)\n","\n","        self.pool = Pooling(pool=pool)\n","        self.classifier = Classifier(dim=emb_dim, num_classes=num_classes)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x.permute(0, 1, 4, 2, 3)\n","        x = self.pad(x).squeeze(1)\n","        b, c, h, w = x.shape\n","        x = F.relu(self.bn_1(self.conv_1(x)))\n","        x = self.res0(x)\n","\n","        x4 = self.patch_embeddings(x)\n","        x5 = self.pos_embeddings(x4)\n","        x6 = self.transformer(x5)\n","\n","        x4_c = x4.reshape(b, -1, h, w)\n","        x_c1 = self.conv1(self.act(self.bn1(x4_c)))\n","        x_c2 = self.conv2(self.act(self.bn2(x_c1)))\n","        x_c3 = self.conv3(self.act(self.bn3(x_c2)))\n","\n","        x7 = self.pool(self.dropout(x6 + x_c3.reshape(b, h*w, -1)))\n","\n","        return self.classifier(x7)\n","\n"]},{"cell_type":"code","source":["x = torch.randn(1, 1, 11, 11, 30)\n","net = CTMixer(channels=30, num_classes=16, image_size=11, datasetname='PaviaU', num_layers=1, num_heads=4)\n","y = net(x)\n","print(y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAMt-tG3mSb2","executionInfo":{"status":"ok","timestamp":1682180775076,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"c419f055-7723-48b4-b53f-219273d10160"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 16])\n"]}]}]}