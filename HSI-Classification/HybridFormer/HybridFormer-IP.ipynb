{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeQS8b4AZfIx5plOD122wg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFJtxl1adiFf","executionInfo":{"status":"ok","timestamp":1677224642692,"user_tz":-480,"elapsed":8441,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"c3635286-e060-4766-a79e-016d513d1f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["URL transformed to HTTPS due to an HSTS policy\n","--2023-02-24 07:43:53--  https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n","Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n","Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5953527 (5.7M)\n","Saving to: ‘Indian_pines_corrected.mat.3’\n","\n","Indian_pines_correc 100%[===================>]   5.68M  1.58MB/s    in 3.6s    \n","\n","2023-02-24 07:43:57 (1.58 MB/s) - ‘Indian_pines_corrected.mat.3’ saved [5953527/5953527]\n","\n","URL transformed to HTTPS due to an HSTS policy\n","--2023-02-24 07:43:57--  https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n","Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n","Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1125 (1.1K)\n","Saving to: ‘Indian_pines_gt.mat.3’\n","\n","Indian_pines_gt.mat 100%[===================>]   1.10K  --.-KB/s    in 0s      \n","\n","2023-02-24 07:43:57 (67.9 MB/s) - ‘Indian_pines_gt.mat.3’ saved [1125/1125]\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spectral in /usr/local/lib/python3.8/dist-packages (0.23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from spectral) (1.22.4)\n"]}],"source":["! wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n","! wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n","! pip install spectral"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io as sio\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n","import spectral\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import copy"],"metadata":{"id":"E-8uiJ1leAxO","executionInfo":{"status":"ok","timestamp":1677224645113,"user_tz":-480,"elapsed":2424,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# 对高光谱数据 X 应用 PCA 变换\n","def applyPCA(X, numComponents):\n","    newX = np.reshape(X, (-1, X.shape[2]))\n","    pca = PCA(n_components=numComponents, whiten=True)\n","    newX = pca.fit_transform(newX)\n","    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n","    return newX\n","\n","# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\n","def padWithZeros(X, margin=2):\n","    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n","    x_offset = margin\n","    y_offset = margin\n","    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n","    return newX\n","\n","# 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\n","def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n","    # 给 X 做 padding\n","    margin = int((windowSize - 1) / 2)\n","    zeroPaddedX = padWithZeros(X, margin=margin)\n","    # split patches\n","    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n","    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n","    patchIndex = 0\n","    for r in range(margin, zeroPaddedX.shape[0] - margin):\n","        for c in range(margin, zeroPaddedX.shape[1] - margin):\n","            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n","            patchesData[patchIndex, :, :, :] = patch\n","            patchesLabels[patchIndex] = y[r-margin, c-margin]\n","            patchIndex = patchIndex + 1\n","    if removeZeroLabels:\n","        patchesData = patchesData[patchesLabels>0,:,:,:]\n","        patchesLabels = patchesLabels[patchesLabels>0]\n","        patchesLabels -= 1\n","    return patchesData, patchesLabels\n","\n","def splitTrainTestSet(X, y, testRatio, randomState=345):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n","    return X_train, X_test, y_train, y_test"],"metadata":{"id":"7UF8Z4PReD_d","executionInfo":{"status":"ok","timestamp":1677224645113,"user_tz":-480,"elapsed":4,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 地物类别\n","class_num = 16\n","X = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n","y = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n","\n","# 用于测试样本的比例\n","test_ratio = 0.90\n","# 每个像素周围提取 patch 的尺寸\n","patch_size = 15\n","# 使用 PCA 降维，得到主成分的数量\n","pca_components = 30\n","\n","print('Hyperspectral data shape: ', X.shape)\n","print('Label shape: ', y.shape)\n","\n","print('\\n... ... PCA tranformation ... ...')\n","X_pca = applyPCA(X, numComponents=pca_components)\n","print('Data shape after PCA: ', X_pca.shape)\n","\n","print('\\n... ... create data cubes ... ...')\n","X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)\n","print('Data cube X shape: ', X_pca.shape)\n","print('Data cube y shape: ', y.shape)\n","\n","print('\\n... ... create train & test data ... ...')\n","Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X_pca, y, test_ratio)\n","print('Xtrain shape: ', Xtrain.shape)\n","print('Xtest  shape: ', Xtest.shape)\n","\n","# 改变 Xtrain, Ytrain 的形状，以符合 keras 的要求\n","Xtrain = Xtrain.reshape(-1, patch_size, patch_size, pca_components, 1)\n","Xtest  = Xtest.reshape(-1, patch_size, patch_size, pca_components, 1)\n","print('before transpose: Xtrain shape: ', Xtrain.shape) \n","print('before transpose: Xtest  shape: ', Xtest.shape) \n","\n","# 为了适应 pytorch 结构，数据要做 transpose\n","Xtrain = Xtrain.transpose(0, 4, 3, 1, 2)\n","Xtest  = Xtest.transpose(0, 4, 3, 1, 2)\n","print('after transpose: Xtrain shape: ', Xtrain.shape) \n","print('after transpose: Xtest  shape: ', Xtest.shape) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeqJl2JQeGHn","executionInfo":{"status":"ok","timestamp":1677224647289,"user_tz":-480,"elapsed":2179,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"4c7bae26-05c7-4d8b-fb4a-271ee64e7e34"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperspectral data shape:  (145, 145, 200)\n","Label shape:  (145, 145)\n","\n","... ... PCA tranformation ... ...\n","Data shape after PCA:  (145, 145, 30)\n","\n","... ... create data cubes ... ...\n","Data cube X shape:  (10249, 15, 15, 30)\n","Data cube y shape:  (10249,)\n","\n","... ... create train & test data ... ...\n","Xtrain shape:  (1024, 15, 15, 30)\n","Xtest  shape:  (9225, 15, 15, 30)\n","before transpose: Xtrain shape:  (1024, 15, 15, 30, 1)\n","before transpose: Xtest  shape:  (9225, 15, 15, 30, 1)\n","after transpose: Xtrain shape:  (1024, 1, 30, 15, 15)\n","after transpose: Xtest  shape:  (9225, 1, 30, 15, 15)\n"]}]},{"cell_type":"code","source":["\"\"\" Training dataset\"\"\"\n","class TrainDS(torch.utils.data.Dataset): \n","    def __init__(self):\n","        self.len = Xtrain.shape[0]\n","        self.x_data = torch.FloatTensor(Xtrain)\n","        self.y_data = torch.LongTensor(ytrain)        \n","    def __getitem__(self, index):\n","        # 根据索引返回数据和对应的标签\n","        return self.x_data[index], self.y_data[index]\n","    def __len__(self): \n","        # 返回文件数据的数目\n","        return self.len\n","\n","\"\"\" Testing dataset\"\"\"\n","class TestDS(torch.utils.data.Dataset): \n","    def __init__(self):\n","        self.len = Xtest.shape[0]\n","        self.x_data = torch.FloatTensor(Xtest)\n","        self.y_data = torch.LongTensor(ytest)\n","    def __getitem__(self, index):\n","        # 根据索引返回数据和对应的标签\n","        return self.x_data[index], self.y_data[index]\n","    def __len__(self): \n","        # 返回文件数据的数目\n","        return self.len\n","\n","# 创建 trainloader 和 testloader\n","trainset = TrainDS()\n","testset  = TestDS()\n","train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=128, shuffle=True, num_workers=2)\n","test_loader  = torch.utils.data.DataLoader(dataset=testset,  batch_size=128, shuffle=False, num_workers=2)"],"metadata":{"id":"kS42sMUGeJGO","executionInfo":{"status":"ok","timestamp":1677224647290,"user_tz":-480,"elapsed":3,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhUkJ8Ehendg","executionInfo":{"status":"ok","timestamp":1677224650908,"user_tz":-480,"elapsed":2723,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"89f58bc5-c75d-45b9-95f4-23f99d6445f3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n"]}]},{"cell_type":"code","source":["import random\n","import numpy as np\n","import torch\n","import cv2\n","import math\n","import torch.nn.functional as F\n","from torch import nn\n","from skimage import transform\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","\n","# helpers\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","\n","class OurFE(nn.Module):\n","    def __init__(self, channel, dim):\n","        super(OurFE, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(channel, channel, kernel_size=1),\n","            nn.BatchNorm2d(channel),\n","            nn.ReLU()\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(channel, channel, kernel_size=1),\n","            nn.BatchNorm2d(channel),\n","            nn.ReLU()\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(channel, channel, kernel_size=1),\n","            nn.BatchNorm2d(channel),\n","            nn.ReLU()\n","        )\n","        self.out_conv = nn.Sequential(\n","            nn.Conv2d(3 * channel, channel, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(channel),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        out1 = self.conv1(x)\n","        out2 = self.conv2(out1)\n","        out3 = self.conv3(out2)\n","        out = self.out_conv(torch.cat((out1, out2, out3), dim=1))\n","        return out\n","\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.fn(self.norm(x))\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            DEPTHWISECONV(dim, 256, kernel_size=3, padding=1, stride=1),\n","            nn.BatchNorm2d(256),\n","            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1),\n","            nn.GELU(),\n","            nn.Conv2d(in_channels=512, out_channels=dim, kernel_size=1),\n","            nn.GELU(),\n","        )\n","\n","    def forward(self, x):\n","        b, d, c = x.shape\n","        w = int(math.sqrt(d))\n","        x1 = rearrange(x, 'b (w h) c -> b c w h', w=w, h=w)\n","        x1 = self.net(x1)\n","        x1 = rearrange(x1, 'b c w h -> b (w h) c')\n","        x = x + x1\n","        return x\n","\n","\n","class DEPTHWISECONV(nn.Module):\n","    def __init__(self, in_ch, out_ch, kernel_size=1, padding=0, stride=1, is_fe=False):\n","        super(DEPTHWISECONV, self).__init__()\n","        self.is_fe = is_fe\n","        self.depth_conv = nn.Conv2d(in_channels=in_ch,\n","                                    out_channels=in_ch,\n","                                    kernel_size=kernel_size,\n","                                    stride=stride,\n","                                    padding=padding,\n","                                    groups=in_ch)\n","        self.point_conv = nn.Conv2d(in_channels=in_ch,\n","                                    out_channels=out_ch,\n","                                    kernel_size=1,\n","                                    stride=1,\n","                                    padding=0,\n","                                    groups=1)\n","\n","    def forward(self, input):\n","        out = self.depth_conv(input)\n","        if self.is_fe:\n","            return out\n","        out = self.point_conv(out)\n","        return out\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads=4, dim_head=64, dropout=0., num_patches=10):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        project_out = not (heads == 1 and dim_head == dim)\n","\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.attend = nn.Softmax(dim=-1)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        ) if project_out else nn.Identity()\n","\n","        self.spatial_norm = nn.BatchNorm2d(heads)\n","        self.spatial_conv = nn.Conv2d(heads, heads, kernel_size=3, padding=1)\n","\n","        self.spectral_norm = nn.BatchNorm2d(1)\n","        self.spectral_conv = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n","        self.to_qkv_spec = nn.Linear(num_patches, num_patches*3, bias=False)\n","        self.attend_spec = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        qkv = self.to_qkv(x).chunk(3, dim=-1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n","        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n","        attn = self.attend(dots)\n","        attn = self.spatial_conv(attn)\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        output = self.to_out(out)\n","\n","        x = x.transpose(-2, -1)\n","        qkv_spec = self.to_qkv_spec(x).chunk(3, dim=-1)\n","        q_spec, k_spec, v_spec = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=1), qkv_spec)\n","        dots_spec = torch.matmul(q_spec, k_spec.transpose(-1, -2)) * self.scale\n","        attn = self.attend_spec(dots_spec)  # .squeeze(dim=1)\n","        attn = self.spectral_conv(attn).squeeze(dim=1)\n","\n","        return torch.matmul(output, attn)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, dropout=0., num_patches=25):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        self.index = 0\n","        for i in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout, num_patches=num_patches)),\n","                PreNorm(dim, FeedForward(dim)),\n","            ]))\n","\n","    def forward(self, x):\n","        output = []\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","            output.append(x)\n","\n","        return x, output\n","\n","\n","class SubNet(nn.Module):\n","    def __init__(self, patch_size, num_patches, dim, emb_dropout, depth, heads, dim_head, mlp_dim, dropout):\n","        super(SubNet, self).__init__()\n","        self.to_patch_embedding = nn.Sequential(\n","            DEPTHWISECONV(in_ch=dim, out_ch=dim, kernel_size = patch_size, stride = patch_size, padding=0, is_fe=True),\n","            Rearrange('b c w h -> b (h w) c '),\n","        )\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches+1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","        self.transformer = Transformer(dim, depth, heads, dim_head, dropout=dropout, num_patches=num_patches)\n","\n","\n","def get_num_patches(ps, ks):\n","    return int((ps - ks)/ks)+1\n","\n","\n","class ViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n","        super(ViT, self).__init__()\n","        self.ournet = OurFE(channels, dim)\n","        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=channels, out_channels=dim, kernel_size=1)\n","        self.net = nn.Sequential()\n","        self.mlp_head = nn.ModuleList()\n","        for ps in patch_size:\n","            num_patches = get_num_patches(image_size, ps) ** 2\n","            patch_dim = dim * num_patches\n","            sub_net = SubNet(ps, num_patches, dim, emb_dropout, depth, heads, dim_head, mlp_dim, dropout)\n","            self.net.append(sub_net)\n","            self.mlp_head.append(nn.Sequential(\n","                nn.LayerNorm(patch_dim),\n","                nn.Linear(patch_dim, num_classes)\n","            ))\n","\n","        self.weight = torch.ones(len(patch_size))\n","\n","    def forward(self, img):\n","        if len(img.shape) == 5: img = img.squeeze()\n","        img = self.ournet(img)\n","        img = self.pool(img)\n","        img = self.conv4(img)\n","\n","        all_branch = []\n","        for sub_branch in self.net:\n","            spatial = sub_branch.to_patch_embedding(img)\n","            b, n, c = spatial.shape\n","            spatial = spatial + sub_branch.pos_embedding[:, :n]\n","            spatial = sub_branch.dropout(spatial)\n","            _, outputs = sub_branch.transformer(spatial)\n","            res = outputs[-1]\n","            all_branch.append(res)\n","\n","        self.weight = F.softmax(self.weight, 0)\n","        res = 0\n","        for i, mlp_head in enumerate(self.mlp_head):\n","            out1 = all_branch[i].flatten(start_dim=1)\n","            cls1 = mlp_head(out1)\n","            res = res + cls1 * self.weight[i]\n","        return res"],"metadata":{"id":"1J4flp6ReQ5k","executionInfo":{"status":"ok","timestamp":1677224650908,"user_tz":-480,"elapsed":4,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["ps=[3, 5]\n","d_h=[4,2]#深度为四，将 transformer encoder循环四次\n","#model = ViT(image_size=15, patch_size=ps, num_classes=16, dim=100, depth=d_h[0], heads=d_h[1],mlp_dim=2048, channels=3, dropout=0.2, emb_dropout=0.2)\n","# 随机输入，测试网络结构是否通\n","x = torch.randn(2, 1, 30, 15, 15)\n","net = ViT(image_size=15, patch_size=ps, num_classes=16, dim=100, depth=d_h[0], heads=d_h[1],mlp_dim=2048, channels=30, dropout=0.2, emb_dropout=0.2)\n","y = net(x)\n","print(y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeuWdEhFe0UU","executionInfo":{"status":"ok","timestamp":1677224650909,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"9d03b041-dcf8-43e3-bc2f-9aacac561d69"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 16])\n"]}]},{"cell_type":"code","source":["def train(net):\n","\n","\n","  current_loss_his = []\n","  current_Acc_his = []\n","\n","  best_net_wts = copy.deepcopy(net.state_dict())\n","  best_acc = 0.0\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","  # 开始训练\n","  total_loss = 0\n","  for epoch in range(100):\n","      net.train()  # 将模型设置为训练模式\n","      for i, (inputs, labels) in enumerate(train_loader):\n","          inputs = inputs.to(device)\n","          labels = labels.to(device)\n","          # 优化器梯度归零\n","          optimizer.zero_grad()\n","          # 正向传播 +　反向传播 + 优化 \n","          outputs = net(inputs)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","          total_loss += loss.item()\n","\n","      if epoch % 10 == 0:\n","        net.eval()   # 将模型设置为验证模式\n","        current_acc = test_acc(net)\n","        current_Acc_his.append(current_acc)\n","        print('[current acc: %.4f]' %(current_acc))\n","\n","      if current_acc > best_acc:\n","        best_acc = current_acc\n","        best_net_wts = copy.deepcopy(net.state_dict())\n","\n","      print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n","      current_loss_his.append(loss.item())\n","\n","  print('Finished Training')\n","  print(\"Best Acc:%.4f\" %(best_acc))\n","\n","  # load best model weights\n","  net.load_state_dict(best_net_wts)\n","\n","  return net,current_loss_his,current_Acc_his"],"metadata":{"id":"KMF9cLiUetwm","executionInfo":{"status":"ok","timestamp":1677224651470,"user_tz":-480,"elapsed":564,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def test_acc(net):\n","  count = 0\n","  # 模型测试\n","  for inputs, _ in test_loader:\n","      inputs = inputs.to(device)\n","      outputs = net(inputs)\n","      outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n","      if count == 0:\n","          y_pred_test =  outputs\n","          count = 1\n","      else:\n","          y_pred_test = np.concatenate( (y_pred_test, outputs) )\n","\n","  # 生成分类报告\n","  classification = classification_report(ytest, y_pred_test, digits=4)\n","  index_acc = classification.find('weighted avg')\n","  accuracy = classification[index_acc+17:index_acc+23]\n","  return float(accuracy)"],"metadata":{"id":"7L3BTNAuevu1","executionInfo":{"status":"ok","timestamp":1677224651470,"user_tz":-480,"elapsed":3,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","# 使用GPU训练，可以在菜单 \"代码执行工具\" -> \"更改运行时类型\" 里进行设置\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 网络放到GPU上\n","net = ViT(image_size=15, patch_size=ps, num_classes=16, dim=100, depth=d_h[0], heads=d_h[1],mlp_dim=2048, channels=30, dropout=0.2, emb_dropout=0.2).to(device)\n","# 训练\n","net,current_loss_his,current_Acc_his = train(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnyPd5gne-YG","executionInfo":{"status":"ok","timestamp":1677224780081,"user_tz":-480,"elapsed":128101,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"6b13824b-94ed-4a70-89f3-96b3488cac8f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[current acc: 0.2135]\n","[Epoch: 1]   [loss avg: 16.4578]   [current loss: 1.5024]\n","[Epoch: 2]   [loss avg: 12.3076]   [current loss: 0.8449]\n","[Epoch: 3]   [loss avg: 9.6352]   [current loss: 0.3861]\n","[Epoch: 4]   [loss avg: 7.8047]   [current loss: 0.2571]\n","[Epoch: 5]   [loss avg: 6.4688]   [current loss: 0.1087]\n","[Epoch: 6]   [loss avg: 5.5065]   [current loss: 0.0464]\n","[Epoch: 7]   [loss avg: 4.7802]   [current loss: 0.0360]\n","[Epoch: 8]   [loss avg: 4.2180]   [current loss: 0.0267]\n","[Epoch: 9]   [loss avg: 3.7687]   [current loss: 0.0204]\n","[Epoch: 10]   [loss avg: 3.4052]   [current loss: 0.0179]\n","[current acc: 0.9818]\n","[Epoch: 11]   [loss avg: 3.1048]   [current loss: 0.0074]\n","[Epoch: 12]   [loss avg: 2.8509]   [current loss: 0.0084]\n","[Epoch: 13]   [loss avg: 2.6404]   [current loss: 0.0553]\n","[Epoch: 14]   [loss avg: 2.4636]   [current loss: 0.0047]\n","[Epoch: 15]   [loss avg: 2.3048]   [current loss: 0.0064]\n","[Epoch: 16]   [loss avg: 2.1647]   [current loss: 0.0096]\n","[Epoch: 17]   [loss avg: 2.0411]   [current loss: 0.0086]\n","[Epoch: 18]   [loss avg: 1.9298]   [current loss: 0.0050]\n","[Epoch: 19]   [loss avg: 1.8297]   [current loss: 0.0043]\n","[Epoch: 20]   [loss avg: 1.7394]   [current loss: 0.0027]\n","[current acc: 0.9865]\n","[Epoch: 21]   [loss avg: 1.6574]   [current loss: 0.0024]\n","[Epoch: 22]   [loss avg: 1.5827]   [current loss: 0.0025]\n","[Epoch: 23]   [loss avg: 1.5145]   [current loss: 0.0012]\n","[Epoch: 24]   [loss avg: 1.4519]   [current loss: 0.0019]\n","[Epoch: 25]   [loss avg: 1.3943]   [current loss: 0.0012]\n","[Epoch: 26]   [loss avg: 1.3410]   [current loss: 0.0009]\n","[Epoch: 27]   [loss avg: 1.2916]   [current loss: 0.0007]\n","[Epoch: 28]   [loss avg: 1.2458]   [current loss: 0.0009]\n","[Epoch: 29]   [loss avg: 1.2032]   [current loss: 0.0010]\n","[Epoch: 30]   [loss avg: 1.1633]   [current loss: 0.0018]\n","[current acc: 0.9875]\n","[Epoch: 31]   [loss avg: 1.1261]   [current loss: 0.0021]\n","[Epoch: 32]   [loss avg: 1.0911]   [current loss: 0.0008]\n","[Epoch: 33]   [loss avg: 1.0583]   [current loss: 0.0006]\n","[Epoch: 34]   [loss avg: 1.0273]   [current loss: 0.0007]\n","[Epoch: 35]   [loss avg: 0.9981]   [current loss: 0.0008]\n","[Epoch: 36]   [loss avg: 0.9706]   [current loss: 0.0007]\n","[Epoch: 37]   [loss avg: 0.9445]   [current loss: 0.0020]\n","[Epoch: 38]   [loss avg: 0.9198]   [current loss: 0.0004]\n","[Epoch: 39]   [loss avg: 0.8963]   [current loss: 0.0006]\n","[Epoch: 40]   [loss avg: 0.8741]   [current loss: 0.0006]\n","[current acc: 0.9875]\n","[Epoch: 41]   [loss avg: 0.8528]   [current loss: 0.0006]\n","[Epoch: 42]   [loss avg: 0.8327]   [current loss: 0.0005]\n","[Epoch: 43]   [loss avg: 0.8135]   [current loss: 0.0012]\n","[Epoch: 44]   [loss avg: 0.7951]   [current loss: 0.0004]\n","[Epoch: 45]   [loss avg: 0.7776]   [current loss: 0.0011]\n","[Epoch: 46]   [loss avg: 0.7607]   [current loss: 0.0006]\n","[Epoch: 47]   [loss avg: 0.7446]   [current loss: 0.0004]\n","[Epoch: 48]   [loss avg: 0.7292]   [current loss: 0.0004]\n","[Epoch: 49]   [loss avg: 0.7144]   [current loss: 0.0003]\n","[Epoch: 50]   [loss avg: 0.7001]   [current loss: 0.0003]\n","[current acc: 0.9872]\n","[Epoch: 51]   [loss avg: 0.6865]   [current loss: 0.0003]\n","[Epoch: 52]   [loss avg: 0.6733]   [current loss: 0.0002]\n","[Epoch: 53]   [loss avg: 0.6607]   [current loss: 0.0007]\n","[Epoch: 54]   [loss avg: 0.6485]   [current loss: 0.0002]\n","[Epoch: 55]   [loss avg: 0.6367]   [current loss: 0.0002]\n","[Epoch: 56]   [loss avg: 0.6254]   [current loss: 0.0002]\n","[Epoch: 57]   [loss avg: 0.6145]   [current loss: 0.0003]\n","[Epoch: 58]   [loss avg: 0.6039]   [current loss: 0.0003]\n","[Epoch: 59]   [loss avg: 0.5937]   [current loss: 0.0002]\n","[Epoch: 60]   [loss avg: 0.5838]   [current loss: 0.0002]\n","[current acc: 0.9874]\n","[Epoch: 61]   [loss avg: 0.5743]   [current loss: 0.0004]\n","[Epoch: 62]   [loss avg: 0.5651]   [current loss: 0.0003]\n","[Epoch: 63]   [loss avg: 0.5561]   [current loss: 0.0002]\n","[Epoch: 64]   [loss avg: 0.5475]   [current loss: 0.0002]\n","[Epoch: 65]   [loss avg: 0.5391]   [current loss: 0.0002]\n","[Epoch: 66]   [loss avg: 0.5310]   [current loss: 0.0002]\n","[Epoch: 67]   [loss avg: 0.5231]   [current loss: 0.0002]\n","[Epoch: 68]   [loss avg: 0.5154]   [current loss: 0.0002]\n","[Epoch: 69]   [loss avg: 0.5079]   [current loss: 0.0001]\n","[Epoch: 70]   [loss avg: 0.5007]   [current loss: 0.0002]\n","[current acc: 0.9879]\n","[Epoch: 71]   [loss avg: 0.4937]   [current loss: 0.0001]\n","[Epoch: 72]   [loss avg: 0.4868]   [current loss: 0.0001]\n","[Epoch: 73]   [loss avg: 0.4802]   [current loss: 0.0001]\n","[Epoch: 74]   [loss avg: 0.4737]   [current loss: 0.0001]\n","[Epoch: 75]   [loss avg: 0.4674]   [current loss: 0.0001]\n","[Epoch: 76]   [loss avg: 0.4613]   [current loss: 0.0002]\n","[Epoch: 77]   [loss avg: 0.4553]   [current loss: 0.0001]\n","[Epoch: 78]   [loss avg: 0.4495]   [current loss: 0.0002]\n","[Epoch: 79]   [loss avg: 0.4438]   [current loss: 0.0001]\n","[Epoch: 80]   [loss avg: 0.4383]   [current loss: 0.0001]\n","[current acc: 0.9877]\n","[Epoch: 81]   [loss avg: 0.4329]   [current loss: 0.0002]\n","[Epoch: 82]   [loss avg: 0.4276]   [current loss: 0.0001]\n","[Epoch: 83]   [loss avg: 0.4225]   [current loss: 0.0002]\n","[Epoch: 84]   [loss avg: 0.4175]   [current loss: 0.0001]\n","[Epoch: 85]   [loss avg: 0.4126]   [current loss: 0.0001]\n","[Epoch: 86]   [loss avg: 0.4078]   [current loss: 0.0001]\n","[Epoch: 87]   [loss avg: 0.4031]   [current loss: 0.0001]\n","[Epoch: 88]   [loss avg: 0.3985]   [current loss: 0.0008]\n","[Epoch: 89]   [loss avg: 0.3941]   [current loss: 0.0001]\n","[Epoch: 90]   [loss avg: 0.3897]   [current loss: 0.0001]\n","[current acc: 0.9882]\n","[Epoch: 91]   [loss avg: 0.3854]   [current loss: 0.0001]\n","[Epoch: 92]   [loss avg: 0.3813]   [current loss: 0.0001]\n","[Epoch: 93]   [loss avg: 0.3772]   [current loss: 0.0001]\n","[Epoch: 94]   [loss avg: 0.3732]   [current loss: 0.0001]\n","[Epoch: 95]   [loss avg: 0.3693]   [current loss: 0.0001]\n","[Epoch: 96]   [loss avg: 0.3654]   [current loss: 0.0001]\n","[Epoch: 97]   [loss avg: 0.3617]   [current loss: 0.0001]\n","[Epoch: 98]   [loss avg: 0.3580]   [current loss: 0.0001]\n","[Epoch: 99]   [loss avg: 0.3544]   [current loss: 0.0001]\n","[Epoch: 100]   [loss avg: 0.3508]   [current loss: 0.0001]\n","Finished Training\n","Best Acc:0.9882\n"]}]},{"cell_type":"code","source":["net.eval()   # 将模型设置为验证模式\n","# 测试最好的模型的结果\n","count = 0\n","# 模型测试\n","for inputs, _ in test_loader:\n","    inputs = inputs.to(device)\n","    outputs = net(inputs)\n","    outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n","    if count == 0:\n","        y_pred_test =  outputs\n","        count = 1\n","    else:\n","        y_pred_test = np.concatenate( (y_pred_test, outputs) )\n","\n","# 生成分类报告\n","classification = classification_report(ytest, y_pred_test, digits=4)\n","print(classification)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8rCDM1t-r2G-","executionInfo":{"status":"ok","timestamp":1677224782493,"user_tz":-480,"elapsed":2416,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"f4c50aa5-08c7-4c74-e8a0-f5f7d52115b8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         0.0     1.0000    1.0000    1.0000        41\n","         1.0     0.9984    0.9533    0.9753      1285\n","         2.0     0.9868    1.0000    0.9934       747\n","         3.0     0.9953    1.0000    0.9977       213\n","         4.0     0.9931    0.9954    0.9943       435\n","         5.0     0.9939    0.9939    0.9939       657\n","         6.0     1.0000    1.0000    1.0000        25\n","         7.0     1.0000    1.0000    1.0000       430\n","         8.0     0.8889    0.8889    0.8889        18\n","         9.0     0.9897    0.9897    0.9897       875\n","        10.0     0.9747    0.9932    0.9839      2210\n","        11.0     0.9943    0.9719    0.9830       534\n","        12.0     1.0000    1.0000    1.0000       185\n","        13.0     0.9939    1.0000    0.9969      1139\n","        14.0     0.9830    1.0000    0.9914       347\n","        15.0     0.9310    0.9643    0.9474        84\n","\n","    accuracy                         0.9881      9225\n","   macro avg     0.9827    0.9844    0.9835      9225\n","weighted avg     0.9882    0.9881    0.9880      9225\n","\n"]}]}]}