{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxQ5tP7ZstAdWKnesijUB5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAYcBUrwo6bj","executionInfo":{"status":"ok","timestamp":1677130345348,"user_tz":-480,"elapsed":8306,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"14c85bf0-6ba1-4714-8737-c0433a5e8f6c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m594.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"o9xJtTGwnzcE","executionInfo":{"status":"ok","timestamp":1677130352399,"user_tz":-480,"elapsed":7054,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","import cv2\n","import math\n","import torch.nn.functional as F\n","from torch import nn\n","from skimage import transform\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","\n","# helpers\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)"]},{"cell_type":"code","source":["class OurFE(nn.Module):\n","  def __init__(self,channel,dim):\n","    super(OurFE,self).__init__()\n","    self.conv1 = nn.Sequential(\n","        nn.Conv2d(channel, channel, kernel_size=1),\n","        nn.BatchNorm2d(channel),\n","        nn.ReLU()\n","    )\n","    self.conv2 = nn.Sequential(\n","        nn.Conv2d(channel, channel, kernel_size=1),\n","        nn.BatchNorm2d(channel),\n","        nn.ReLU()\n","    )\n","    self.conv3 = nn.Sequential(\n","        nn.Conv2d(channel, channel, kernel_size=1),\n","        nn.BatchNorm2d(channel),\n","        nn.ReLU()\n","    )\n","    self.out_conv = nn.Sequential(\n","        nn.Conv2d(3*channel,channel,kernel_size=3,padding=1),\n","        nn.BatchNorm2d(channel),\n","        nn.ReLU()\n","    )\n","  def forward(self,x):\n","    out1 = self.conv1(x)\n","    out2 = self.conv2(out1)\n","    out3 = self.conv3(out2)\n","    out = self.out_conv(torch.cat((out1,out2,out3),dim=1))\n","    return out"],"metadata":{"id":"lUvAtN9kpcdi","executionInfo":{"status":"ok","timestamp":1677130352399,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.fn(self.norm(x))"],"metadata":{"id":"nt1lKk_nr6Vq","executionInfo":{"status":"ok","timestamp":1677130352400,"user_tz":-480,"elapsed":6,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","  def __init__(self,dim):\n","    super().__init__()\n","    self.net = nn.Sequential(\n","        DEPTHWISECONV(dim,256,kernel_size=3,padding=1,stride=1),\n","        nn.BatchNorm2d(256),\n","        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1),\n","        nn.GELU(),\n","        nn.Conv2d(in_channels=512, out_channels=dim, kernel_size=1),\n","        nn.GELU(),\n","    )\n","  def forward(self,x):\n","    b,d,c=x.shape\n","    w = int(math.sqrt(d))\n","    x1 = rearrange(x, 'b (w h) c -> b c w h', w=w, h=w)\n","    x1 = self.net(x1)\n","    x1 = rearrange(x1, 'b c w h -> b (w h) c')\n","    x = x + x1\n","    return x"],"metadata":{"id":"zhKroZejsFfL","executionInfo":{"status":"ok","timestamp":1677130352400,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class DEPTHWISECONV(nn.Module):\n","  def __init__(self,in_ch,out_ch,kernel_size=1,padding=0,stride=1,is_fe=False):\n","    super(DEPTHWISECONV,self).__init__()\n","    self.is_fe = is_fe\n","    self.depth_conv = nn.Conv2d(\n","        in_channels=in_ch,\n","        out_channels=in_ch,\n","        kernel_size=kernel_size,\n","        stride=stride,\n","        padding=padding,\n","        groups=in_ch\n","    )\n","    self.point_conv = nn.Conv2d(\n","        in_channels=in_ch,\n","        out_channels=out_ch,\n","        kernel_size=1,\n","        stride=1,\n","        padding=0,\n","        groups=1\n","    )\n","  def forward(self,input):\n","    out = self.depth_conv(input)\n","    if self.is_fe:\n","      return out\n","    out = self.point_conv(out)\n","    return out"],"metadata":{"id":"TyXFmUpVuRq6","executionInfo":{"status":"ok","timestamp":1677130352400,"user_tz":-480,"elapsed":5,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):\n","  def __init__(self,dim,heads=4,dim_head=64,dropout=0.,num_patches=10):\n","    super().__init__()\n","    inner_dim = dim_head*heads\n","    project_out = not(heads==1 and dim_head==dim)\n","\n","    self.heads = heads\n","    self.scale = dim_head ** -0.5\n","    self.attend = nn.Softmax(dim=-1)\n","    self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","    self.to_out = nn.Sequential(\n","        nn.Linear(inner_dim,dim),\n","        nn.Dropout(dropout)\n","    ) if project_out else nn.Identity()\n","\n","    self.spatial_norm = nn.BatchNorm2d(heads)\n","    self.spatial_conv = nn.Conv2d(heads, heads, kernel_size=3, padding=1)\n","\n","    self.spectral_norm = nn.BatchNorm2d(1)\n","    self.spectral_conv = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n","    self.to_qkv_spec = nn.Linear(dim, dim*3, bias=False)\n","    self.attend_spec = nn.Softmax(dim=-1)\n","  def forward(self,x):\n","    print(\".........spaAttention\")\n","    qkv = self.to_qkv(x).chunk(3,dim=-1)\n","    #print(\".........qkv.shape:\",qkv.shape)\n","    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n","    print(\".........q.shape：{} | k.shape:{} | v.shape:{}\".format(str(q.shape),str(k.shape),str(v.shape)))\n","    dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n","    print(\".........dots_spa.shape:\",dots.shape)\n","    attn = self.attend(dots)\n","    attn = self.spatial_conv(attn)\n","    print(\".........afetr conv2d attn_spa.shape:\",attn.shape)\n","    out = torch.matmul(attn, v)\n","    print(\".........out_spa.shape:\",out.shape)\n","    out = rearrange(out, 'b h n d -> b n (h d)')\n","    print(\".........after rearrange out.spa.shape:\",out.shape)\n","    output = self.to_out(out)\n","    print(\".........final out.spa.shape:\",output.shape)\n","\n","\n","    print(\".........speAttention\")\n","    #x = x.transpose(-2, -1)\n","    print(\".........speAttention inputdata x.shape:\",x.shape)\n","    qkv_spec = self.to_qkv_spec(x).chunk(3, dim=-1)\n","    q_spec, k_spec, v_spec = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=1), qkv_spec)\n","\n","    print(\".........q_spec.shape：{} | k_spec.shape:{} | v_spec.shape:{}\".format(str(q_spec.shape),str(k_spec.shape),str(v_spec.shape)))\n","    dots_spec = torch.matmul(q_spec.transpose(-1, -2), k_spec) * self.scale\n","    print(\".........dots_spec.shape:\",dots_spec.shape)\n","    attn = self.attend_spec(dots_spec)  # .squeeze(dim=1)\n","    print(\".........attend_spec.shape:\",attn.shape)\n","    attn = self.spectral_conv(attn)\n","    print(\".........afetr attend_spec.shape:\",attn.shape)\n","    out_spec = torch.matmul(attn, v_spec.transpose(-2, -1))\n","    out_spec = out_spec.squeeze(dim=1)\n","    out_spec = out_spec.transpose(-2, -1)\n","    print(\".........out_spec.shape:\",out_spec.shape)\n","\n","    #out_final = torch.matmul(output, out_spec).squeeze(dim=1)\n","    out_final = output+out_spec\n","    print(\".........out_final.shape:\",out_final.shape)\n","\n","    return out_final"],"metadata":{"id":"kZm7G50GxZt6","executionInfo":{"status":"ok","timestamp":1677130575893,"user_tz":-480,"elapsed":2,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, dropout=0., num_patches=25):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        self.index = 0\n","        for i in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout, num_patches=num_patches)),\n","                PreNorm(dim, FeedForward(dim)),\n","            ]))\n","\n","    def forward(self, x):\n","        print(\"...attentionModule input x.shape:\",x.shape)\n","        output = []\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            print(\"...attentionModule after attn x.shape:\",x.shape)\n","            x = ff(x) + x\n","            print(\"...attentionModule after FeedForward x.shape:\",x.shape)\n","            output.append(x)\n","            print(\"...the len of output is:\",len(output))\n","\n","        return x, output#output是将每层transformerencoder的结果都储存了下来。"],"metadata":{"id":"nIc22yHX5whj","executionInfo":{"status":"ok","timestamp":1677130362255,"user_tz":-480,"elapsed":809,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class SubNet(nn.Module):\n","  def __init__(self,patch_size,num_patches,dim,emb_dropout,depth,heads,dim_head,mlp_dim,dropout):\n","    super(SubNet,self).__init__()\n","    self.to_patch_embedding = nn.Sequential(\n","        DEPTHWISECONV(in_ch=dim, out_ch=dim, kernel_size = patch_size, stride = patch_size, padding=0, is_fe=True),\n","        Rearrange('b c w h -> b (h w) c '),\n","    )\n","    self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n","    self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches+1, dim))\n","    self.dropout = nn.Dropout(emb_dropout)\n","    self.transformer = Transformer(dim, depth, heads, dim_head, dropout=dropout, num_patches=num_patches)\n"],"metadata":{"id":"UNhpLSSW7pG0","executionInfo":{"status":"ok","timestamp":1677130363867,"user_tz":-480,"elapsed":2,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_num_patches(ps, ks):\n","    return int((ps - ks)/ks)+1"],"metadata":{"id":"btNNIz3T-7vr","executionInfo":{"status":"ok","timestamp":1677130366414,"user_tz":-480,"elapsed":2,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class ViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n","        super(ViT, self).__init__()\n","        self.ournet = OurFE(channels, dim)\n","        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=channels, out_channels=dim, kernel_size=1)\n","        self.net = nn.Sequential()#多个subnet的集合\n","        self.mlp_head = nn.ModuleList()#多个分类头的集合\n","        for ps in patch_size:# subpatch size\n","          #如果将patch_size设置为【3，5】这会产生两个子网络。子网络包含MDTE模块\n","          num_patches = get_num_patches\n","          num_patches = get_num_patches(image_size, ps) ** 2\n","          patch_dim = dim * num_patches\n","          print(\"ps:{}, num_patches:{}, dim:{}, patch_dim:{}\".format(ps,num_patches,dim,patch_dim))\n","          sub_net = SubNet(ps, num_patches, dim, emb_dropout, depth, heads, dim_head, mlp_dim, dropout)\n","          self.net.append(sub_net)\n","          self.mlp_head.append(nn.Sequential(\n","              nn.LayerNorm(patch_dim),\n","              nn.Linear(patch_dim,num_classes)\n","          ))\n","        self.weight = torch.ones(len(patch_size))\n","    def forward(self,img):\n","      #if len(img.shape) == 5: img = img.squeeze()\n","      print(\"input data img.shape:\",img.shape)\n","      img = self.ournet(img)# CNN feature extractor\n","      print(\"after FE img.shape:\",img.shape)\n","      img = self.pool(img)\n","      print(\"after pool img.shape:\",img.shape)\n","      img = self.conv4(img)\n","      print(\"after conv4 img.shape:\",img.shape)\n","      print(\"****************************【FeatureExtraction finish】*******************************\")\n","      all_branch = []\n","      ith_layer=1\n","      for sub_branch in self.net:\n","        spatial = sub_branch.to_patch_embedding(img)#多粒度深度卷积标记嵌入（MDTE）模块  这一步会得到token\n","        print(\"after to_patch_embedding spatial.shape:\",spatial.shape)\n","        b, n, c = spatial.shape\n","        print(\"b:{} | n:{} | c:{}\".format(b,n,c))\n","        print(\"****************************【tokenlize finish】*******************************\")\n","        spatial = spatial + sub_branch.pos_embedding[:, :n]\n","        print(\"(spatial + sub_branch.pos_embedding).shape:\",spatial.shape)\n","        spatial = sub_branch.dropout(spatial)\n","        _, outputs = sub_branch.transformer(spatial)\n","        #print(\"after transformer outputs:\",outputs)\n","        res = outputs[-1]#选取最后一层transformer encoder的输出作为res【transformerencoder的层数就是depth,output会保存每层的输出】【为什么不直接输出最后一个结果呢？？？】\n","        print(\"res.shape\",res.shape)\n","        all_branch.append(res)#将每一个子网络的结果存入列表\n","        print(\"第{}支子网络运行完毕！\".format(ith_layer))\n","        ith_layer+=1\n","\n","      print(\"****************************【TransformerEncoder finish】*******************************\")\n","      print(\"all_branch:\",len(all_branch))\n","      self.weight = F.softmax(self.weight, 0)\n","      res = 0\n","      for i, mlp_head in enumerate(self.mlp_head):\n","        out1 = all_branch[i].flatten(start_dim=1)\n","        print(\"压缩第{}个子网络的输出，压缩后的维度为：{}\".format(int(i+1),str(out1.shape)))\n","        cls1 = mlp_head(out1)\n","        print(\"分类头处理第{}个子网络的压缩输出，处理后的维度为：{}\".format(int(i+1),str(cls1.shape)))\n","        res = res + cls1 * self.weight[i]\n","      print(\"****************************【classification finish】*******************************\")\n","      return res"],"metadata":{"id":"JTvBbosdAoC0","executionInfo":{"status":"ok","timestamp":1677130367556,"user_tz":-480,"elapsed":2,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["ps=[3, 5]\n","d_h=[4,2]#深度为四，将 transformer encoder循环四次\n","#model = ViT(image_size=15, patch_size=ps, num_classes=16, dim=100, depth=d_h[0], heads=d_h[1],mlp_dim=2048, channels=3, dropout=0.2, emb_dropout=0.2)\n","# 随机输入，测试网络结构是否通\n","x = torch.randn(1, 30, 15, 15)\n","net = ViT(image_size=15, patch_size=ps, num_classes=16, dim=100, depth=d_h[0], heads=d_h[1],mlp_dim=2048, channels=30, dropout=0.2, emb_dropout=0.2)\n","y = net(x)\n","print(y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXyQYeUIRve2","executionInfo":{"status":"ok","timestamp":1677130578685,"user_tz":-480,"elapsed":4,"user":{"displayName":"feyyy chow","userId":"01991168991301125311"}},"outputId":"254e9d64-3e31-42f8-ebf0-019e6c8e34f2"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["ps:3, num_patches:25, dim:100, patch_dim:2500\n","ps:5, num_patches:9, dim:100, patch_dim:900\n","input data img.shape: torch.Size([1, 30, 15, 15])\n","after FE img.shape: torch.Size([1, 30, 15, 15])\n","after pool img.shape: torch.Size([1, 30, 15, 15])\n","after conv4 img.shape: torch.Size([1, 100, 15, 15])\n","****************************【FeatureExtraction finish】*******************************\n","after to_patch_embedding spatial.shape: torch.Size([1, 25, 100])\n","b:1 | n:25 | c:100\n","****************************【tokenlize finish】*******************************\n","(spatial + sub_branch.pos_embedding).shape: torch.Size([1, 25, 100])\n","...attentionModule input x.shape: torch.Size([1, 25, 100])\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 25, 64]) | k.shape:torch.Size([1, 2, 25, 64]) | v.shape:torch.Size([1, 2, 25, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 25, 25])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 25, 25])\n",".........out_spa.shape: torch.Size([1, 2, 25, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 25, 128])\n",".........final out.spa.shape: torch.Size([1, 25, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 25, 100])\n",".........q_spec.shape：torch.Size([1, 1, 25, 100]) | k_spec.shape:torch.Size([1, 1, 25, 100]) | v_spec.shape:torch.Size([1, 1, 25, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 25, 100])\n",".........out_final.shape: torch.Size([1, 25, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 25, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 25, 100])\n","...the len of output is: 1\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 25, 64]) | k.shape:torch.Size([1, 2, 25, 64]) | v.shape:torch.Size([1, 2, 25, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 25, 25])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 25, 25])\n",".........out_spa.shape: torch.Size([1, 2, 25, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 25, 128])\n",".........final out.spa.shape: torch.Size([1, 25, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 25, 100])\n",".........q_spec.shape：torch.Size([1, 1, 25, 100]) | k_spec.shape:torch.Size([1, 1, 25, 100]) | v_spec.shape:torch.Size([1, 1, 25, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 25, 100])\n",".........out_final.shape: torch.Size([1, 25, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 25, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 25, 100])\n","...the len of output is: 2\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 25, 64]) | k.shape:torch.Size([1, 2, 25, 64]) | v.shape:torch.Size([1, 2, 25, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 25, 25])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 25, 25])\n",".........out_spa.shape: torch.Size([1, 2, 25, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 25, 128])\n",".........final out.spa.shape: torch.Size([1, 25, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 25, 100])\n",".........q_spec.shape：torch.Size([1, 1, 25, 100]) | k_spec.shape:torch.Size([1, 1, 25, 100]) | v_spec.shape:torch.Size([1, 1, 25, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 25, 100])\n",".........out_final.shape: torch.Size([1, 25, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 25, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 25, 100])\n","...the len of output is: 3\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 25, 64]) | k.shape:torch.Size([1, 2, 25, 64]) | v.shape:torch.Size([1, 2, 25, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 25, 25])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 25, 25])\n",".........out_spa.shape: torch.Size([1, 2, 25, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 25, 128])\n",".........final out.spa.shape: torch.Size([1, 25, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 25, 100])\n",".........q_spec.shape：torch.Size([1, 1, 25, 100]) | k_spec.shape:torch.Size([1, 1, 25, 100]) | v_spec.shape:torch.Size([1, 1, 25, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 25, 100])\n",".........out_final.shape: torch.Size([1, 25, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 25, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 25, 100])\n","...the len of output is: 4\n","res.shape torch.Size([1, 25, 100])\n","第1支子网络运行完毕！\n","after to_patch_embedding spatial.shape: torch.Size([1, 9, 100])\n","b:1 | n:9 | c:100\n","****************************【tokenlize finish】*******************************\n","(spatial + sub_branch.pos_embedding).shape: torch.Size([1, 9, 100])\n","...attentionModule input x.shape: torch.Size([1, 9, 100])\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 9, 64]) | k.shape:torch.Size([1, 2, 9, 64]) | v.shape:torch.Size([1, 2, 9, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 9, 9])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 9, 9])\n",".........out_spa.shape: torch.Size([1, 2, 9, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 9, 128])\n",".........final out.spa.shape: torch.Size([1, 9, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 9, 100])\n",".........q_spec.shape：torch.Size([1, 1, 9, 100]) | k_spec.shape:torch.Size([1, 1, 9, 100]) | v_spec.shape:torch.Size([1, 1, 9, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 9, 100])\n",".........out_final.shape: torch.Size([1, 9, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 9, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 9, 100])\n","...the len of output is: 1\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 9, 64]) | k.shape:torch.Size([1, 2, 9, 64]) | v.shape:torch.Size([1, 2, 9, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 9, 9])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 9, 9])\n",".........out_spa.shape: torch.Size([1, 2, 9, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 9, 128])\n",".........final out.spa.shape: torch.Size([1, 9, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 9, 100])\n",".........q_spec.shape：torch.Size([1, 1, 9, 100]) | k_spec.shape:torch.Size([1, 1, 9, 100]) | v_spec.shape:torch.Size([1, 1, 9, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 9, 100])\n",".........out_final.shape: torch.Size([1, 9, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 9, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 9, 100])\n","...the len of output is: 2\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 9, 64]) | k.shape:torch.Size([1, 2, 9, 64]) | v.shape:torch.Size([1, 2, 9, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 9, 9])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 9, 9])\n",".........out_spa.shape: torch.Size([1, 2, 9, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 9, 128])\n",".........final out.spa.shape: torch.Size([1, 9, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 9, 100])\n",".........q_spec.shape：torch.Size([1, 1, 9, 100]) | k_spec.shape:torch.Size([1, 1, 9, 100]) | v_spec.shape:torch.Size([1, 1, 9, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 9, 100])\n",".........out_final.shape: torch.Size([1, 9, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 9, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 9, 100])\n","...the len of output is: 3\n",".........spaAttention\n",".........q.shape：torch.Size([1, 2, 9, 64]) | k.shape:torch.Size([1, 2, 9, 64]) | v.shape:torch.Size([1, 2, 9, 64])\n",".........dots_spa.shape: torch.Size([1, 2, 9, 9])\n",".........afetr conv2d attn_spa.shape: torch.Size([1, 2, 9, 9])\n",".........out_spa.shape: torch.Size([1, 2, 9, 64])\n",".........after rearrange out.spa.shape: torch.Size([1, 9, 128])\n",".........final out.spa.shape: torch.Size([1, 9, 100])\n",".........speAttention\n",".........speAttention inputdata x.shape: torch.Size([1, 9, 100])\n",".........q_spec.shape：torch.Size([1, 1, 9, 100]) | k_spec.shape:torch.Size([1, 1, 9, 100]) | v_spec.shape:torch.Size([1, 1, 9, 100])\n",".........dots_spec.shape: torch.Size([1, 1, 100, 100])\n",".........attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........afetr attend_spec.shape: torch.Size([1, 1, 100, 100])\n",".........out_spec.shape: torch.Size([1, 9, 100])\n",".........out_final.shape: torch.Size([1, 9, 100])\n","...attentionModule after attn x.shape: torch.Size([1, 9, 100])\n","...attentionModule after FeedForward x.shape: torch.Size([1, 9, 100])\n","...the len of output is: 4\n","res.shape torch.Size([1, 9, 100])\n","第2支子网络运行完毕！\n","****************************【TransformerEncoder finish】*******************************\n","all_branch: 2\n","压缩第1个子网络的输出，压缩后的维度为：torch.Size([1, 2500])\n","分类头处理第1个子网络的压缩输出，处理后的维度为：torch.Size([1, 16])\n","压缩第2个子网络的输出，压缩后的维度为：torch.Size([1, 900])\n","分类头处理第2个子网络的压缩输出，处理后的维度为：torch.Size([1, 16])\n","****************************【classification finish】*******************************\n","torch.Size([1, 16])\n"]}]}]}